{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Stateful Processing",
        "description": "Add support for stateful stream processing where transformers can maintain and update state across items.",
        "status": "done",
        "priority": "high",
        "dependencies": [],
        "details": "## Context\n\nStateful processing is essential for many stream processing use cases like aggregations, sessionization, and pattern detection. Currently, StreamWeave transformers are stateless - they process each item independently without memory of previous items.\n\n## Implementation Approach\n\n1. Create a `StatefulTransformer` trait extending the base `Transformer` trait\n2. Add state storage abstraction (in-memory initially, with hooks for external stores)\n3. Implement state lifecycle management (initialization, updates, cleanup)\n4. Add state serialization for checkpointing\n\n## Technical Considerations\n\n- State must be thread-safe for concurrent access\n- Memory management for large state\n- State migration between transformer versions\n- Integration with error handling strategies\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] `StatefulTransformer` trait is defined with state type parameter\n- [ ] State can be initialized, read, updated, and cleared\n- [ ] State persists across multiple items in the stream\n- [ ] State is properly cleaned up when pipeline completes\n- [ ] Running sum/average transformer example works correctly\n- [ ] State snapshots can be taken and restored\n- [ ] Unit tests achieve >90% coverage\n- [ ] Documentation includes usage examples\n- [ ] Performance benchmark shows <10% overhead vs stateless\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Design StatefulTransformer trait",
            "description": "Design and implement the core StatefulTransformer trait with state type parameter",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nThe StatefulTransformer trait needs to extend the existing Transformer trait while adding state management capabilities. It should be generic over the state type and provide methods for state access.\n\n## Requirements\n\n- Generic state type parameter with appropriate bounds (Send, Sync, Clone)\n- Methods: get_state(), update_state(), reset_state()\n- Integration with existing transform() method\n- State initialization hook\n\n## Acceptance Criteria\n\n- [ ] Trait compiles and is ergonomic to implement\n- [ ] State type is generic and flexible\n- [ ] Trait is compatible with existing pipeline builder\n- [ ] Documentation explains state lifecycle\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement in-memory state storage",
            "description": "Create thread-safe in-memory state storage implementation",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nThe default state storage should be in-memory using appropriate Rust synchronization primitives.\n\n## Requirements\n\n- Use Arc<RwLock<S>> or similar for thread-safe access\n- Support atomic state updates\n- Implement StateStore trait for abstraction\n\n## Acceptance Criteria\n\n- [ ] Thread-safe concurrent read/write access\n- [ ] No deadlocks under concurrent access patterns\n- [ ] Memory is properly freed on cleanup\n- [ ] Performance is acceptable for high-throughput streams\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Create example stateful transformers",
            "description": "Implement RunningSum, MovingAverage, and SessionWindow as example stateful transformers",
            "status": "done",
            "dependencies": [
              1,
              2
            ],
            "details": "## Context\n\nExample implementations help users understand the API and serve as integration tests.\n\n## Requirements\n\n- RunningSumTransformer: maintains cumulative sum\n- MovingAverageTransformer: configurable window size\n- Add to transformers module\n\n## Acceptance Criteria\n\n- [ ] All examples compile and work correctly\n- [ ] Examples are well-documented with doc comments\n- [ ] Examples appear in generated documentation\n- [ ] Tests verify correct behavior\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Add state checkpointing",
            "description": "Implement state serialization and checkpointing for recovery",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nCheckpointing allows state to be saved and restored for fault tolerance.\n\n## Requirements\n\n- Serialize state to bytes using serde\n- Save checkpoints to configurable location\n- Restore state from checkpoint on startup\n\n## Acceptance Criteria\n\n- [ ] State can be serialized to JSON/bincode\n- [ ] Checkpoints can be saved to file\n- [ ] State can be restored from checkpoint\n- [ ] Checkpoint interval is configurable\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Exactly-Once Processing",
        "description": "Implement exactly-once processing guarantees for reliable stream processing with deduplication and idempotency.",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "## Context\n\nExactly-once semantics ensure each record is processed exactly once, even in the presence of failures. This is critical for financial transactions, inventory updates, and other business-critical operations.\n\n## Implementation Approach\n\n1. Implement message IDs and deduplication\n2. Add transaction support for atomic commits\n3. Implement offset tracking for resumable processing\n4. Add checkpointing integration\n\n## Technical Considerations\n\n- Trade-off between latency and exactly-once guarantees\n- Storage requirements for deduplication state\n- Integration with external systems (databases, message queues)\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Each message has a unique identifier\n- [ ] Duplicate messages are detected and skipped\n- [ ] Pipeline can resume from last checkpoint after crash\n- [ ] Offset tracking persists across restarts\n- [ ] Transaction rollback on failure works correctly\n- [ ] No data loss or duplication in failure scenarios\n- [ ] Performance impact is documented\n- [ ] Integration tests cover failure scenarios\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement message ID tracking",
            "description": "Add unique message IDs to stream items for deduplication",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMessage IDs are the foundation of exactly-once processing.\n\n## Requirements\n\n- Generate unique IDs (UUID or sequence)\n- Attach ID to each stream item\n- Make ID accessible in transformers\n\n## Acceptance Criteria\n\n- [ ] All items have unique IDs\n- [ ] IDs are preserved through transformations\n- [ ] IDs can be extracted for deduplication\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Create deduplication transformer",
            "description": "Implement a transformer that filters duplicate messages based on ID",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nDeduplication removes repeated messages that may occur due to retries.\n\n## Requirements\n\n- Track seen message IDs\n- Configurable dedup window (time or count based)\n- Memory-efficient ID storage (bloom filter option)\n\n## Acceptance Criteria\n\n- [ ] Duplicates are correctly identified and filtered\n- [ ] Window expiration works correctly\n- [ ] Memory usage is bounded\n- [ ] Performance is acceptable for high-throughput\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement offset tracking",
            "description": "Track processing offsets for resumable pipelines",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nOffset tracking allows pipelines to resume from where they left off.\n\n## Requirements\n\n- Track last processed offset per source\n- Persist offsets to storage\n- Support offset commit strategies (auto, manual)\n\n## Acceptance Criteria\n\n- [ ] Offsets are tracked accurately\n- [ ] Offsets persist across restarts\n- [ ] Manual and auto commit modes work\n- [ ] Offset reset (earliest/latest) is supported\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Add transaction support",
            "description": "Implement transactional processing for atomic commits",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nTransactions ensure all-or-nothing processing for batches.\n\n## Requirements\n\n- Transaction begin/commit/rollback API\n- Integration with offset commits\n- Configurable transaction timeout\n\n## Acceptance Criteria\n\n- [ ] Transactions can be started and committed\n- [ ] Rollback undoes partial processing\n- [ ] Timeouts prevent stuck transactions\n- [ ] Nested transactions handled correctly\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Windowing Operations",
        "description": "Implement comprehensive windowing operations for time-based and count-based stream processing.",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "## Context\n\nWindowing is fundamental to stream processing, allowing bounded computations over unbounded streams. Windows group elements for aggregation based on time or count.\n\n## Implementation Approach\n\n1. Define Window trait and common window types\n2. Implement tumbling, sliding, and session windows\n3. Add watermark support for event-time processing\n4. Handle late data with configurable policies\n\n## Technical Considerations\n\n- Memory management for window state\n- Timer management for window triggers\n- Trade-offs between latency and completeness\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Tumbling windows work with time and count triggers\n- [ ] Sliding windows handle overlapping correctly\n- [ ] Session windows detect gaps and close appropriately\n- [ ] Watermarks track event-time progress\n- [ ] Late data is handled per configured policy\n- [ ] Window state is garbage collected after closing\n- [ ] Performance scales with window count\n- [ ] Examples demonstrate each window type\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Window trait and types",
            "description": "Create the core Window trait and enum for window types",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nA well-designed Window abstraction enables flexible windowing strategies.\n\n## Requirements\n\n- Window trait with assign(), trigger(), merge() methods\n- WindowAssigner for assigning elements to windows\n- WindowTrigger for determining when to emit\n\n## Acceptance Criteria\n\n- [ ] Trait is flexible enough for all window types\n- [ ] Type system prevents invalid window configurations\n- [ ] API is ergonomic and well-documented\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement TumblingWindow",
            "description": "Create non-overlapping fixed-size windows",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nTumbling windows are the simplest window type - fixed size, non-overlapping.\n\n## Requirements\n\n- Time-based tumbling windows\n- Count-based tumbling windows\n- Configurable window size\n\n## Acceptance Criteria\n\n- [ ] Windows don't overlap\n- [ ] All elements assigned to exactly one window\n- [ ] Window closes after trigger\n- [ ] Late elements handled per policy\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement SlidingWindow",
            "description": "Create overlapping windows with configurable slide",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nSliding windows allow elements to belong to multiple windows.\n\n## Requirements\n\n- Configurable window size and slide interval\n- Elements assigned to all applicable windows\n- Efficient memory management\n\n## Acceptance Criteria\n\n- [ ] Elements appear in correct number of windows\n- [ ] Slide interval works correctly\n- [ ] Memory doesn't grow unbounded\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement SessionWindow",
            "description": "Create gap-based session windows",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nSession windows group elements with gaps smaller than a threshold.\n\n## Requirements\n\n- Configurable session gap timeout\n- Dynamic window boundaries\n- Session merging when gaps fill in\n\n## Acceptance Criteria\n\n- [ ] Sessions close after gap timeout\n- [ ] Adjacent sessions merge correctly\n- [ ] Late arrivals extend sessions properly\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 5,
            "title": "Implement watermarks and late data handling",
            "description": "Add event-time tracking and late data policies",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "## Context\n\nWatermarks track progress in event time, enabling handling of out-of-order data.\n\n## Requirements\n\n- Watermark generation strategies\n- Late data policies (drop, emit, sideOutput)\n- Allowed lateness configuration\n\n## Acceptance Criteria\n\n- [ ] Watermarks advance correctly\n- [ ] Late data identified accurately\n- [ ] All policies implemented and tested\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 4,
        "title": "Add Specialized Transformers",
        "description": "Implement additional specialized transformers for common stream processing operations.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nA rich set of transformers makes the framework more useful out of the box. These are common operations that users shouldn't need to implement themselves.\n\n## Implementation Approach\n\n1. Implement each transformer following existing patterns\n2. Ensure consistent API and error handling\n3. Add comprehensive tests and documentation\n\n## Technical Considerations\n\n- Maintain consistency with existing transformer API\n- Consider memory usage for stateful transformers\n- Ensure WASM compatibility\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] All planned transformers implemented\n- [ ] Each transformer has >90% test coverage\n- [ ] All transformers have doc comments with examples\n- [ ] Performance benchmarks for each transformer\n- [ ] All transformers work in WASM builds\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FlatMap transformer",
            "description": "Create transformer that maps one input to zero or more outputs",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nFlatMap is essential for one-to-many transformations like splitting strings.\n\n## Requirements\n\n- Accept function that returns iterator\n- Flatten results into output stream\n- Handle empty results gracefully\n\n## Acceptance Criteria\n\n- [ ] One-to-many mapping works correctly\n- [ ] Empty results don't break stream\n- [ ] Type inference works well\n- [ ] Performance is comparable to manual implementation\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement Reduce transformer",
            "description": "Create transformer for aggregating stream elements",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nReduce combines elements using an associative function.\n\n## Requirements\n\n- Accept binary reduction function\n- Support initial accumulator value\n- Emit intermediate or final results\n\n## Acceptance Criteria\n\n- [ ] Reduction produces correct result\n- [ ] Works with various types\n- [ ] Can emit running results if configured\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement GroupBy transformer",
            "description": "Create transformer for grouping elements by key",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nGroupBy partitions streams by key for per-key processing.\n\n## Requirements\n\n- Accept key extraction function\n- Route elements to per-key sub-streams\n- Support downstream per-key aggregation\n\n## Acceptance Criteria\n\n- [ ] Elements grouped by correct key\n- [ ] Per-key processing is isolated\n- [ ] Memory bounded for many keys\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement Join transformer",
            "description": "Create transformer for joining two streams",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nJoins combine elements from two streams based on keys.\n\n## Requirements\n\n- Inner join implementation\n- Configurable join window\n- Key-based matching\n\n## Acceptance Criteria\n\n- [ ] Matching elements are correctly joined\n- [ ] Window limits memory usage\n- [ ] Unmatched elements handled per config\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 5,
            "title": "Implement Distinct transformer",
            "description": "Create transformer that emits only unique elements",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDistinct removes duplicates from the stream.\n\n## Requirements\n\n- Track seen elements\n- Configurable uniqueness key\n- Memory-bounded tracking (optional bloom filter)\n\n## Acceptance Criteria\n\n- [ ] Duplicates correctly filtered\n- [ ] Custom key extraction works\n- [ ] Memory usage is bounded\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 6,
            "title": "Implement Sample transformer",
            "description": "Create transformer for statistical sampling",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nSampling reduces stream volume for testing or analysis.\n\n## Requirements\n\n- Random sampling with configurable rate\n- Reservoir sampling for fixed-size samples\n- Reproducible with seed\n\n## Acceptance Criteria\n\n- [ ] Sample rate is accurate over time\n- [ ] Distribution is uniform\n- [ ] Seeded sampling is reproducible\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Fan-In/Fan-Out Support",
        "description": "Add support for splitting streams to multiple consumers and merging multiple streams.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nFan-out allows one stream to feed multiple downstream processors. Fan-in merges multiple streams into one. These are essential for parallel processing and stream routing.\n\n## Implementation Approach\n\n1. Design stream splitting mechanism\n2. Implement various distribution strategies\n3. Add stream merging with ordering options\n4. Create routing based on content\n\n## Technical Considerations\n\n- Backpressure propagation across branches\n- Ordering guarantees in fan-in\n- Memory management for buffering\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Stream can be split to multiple consumers\n- [ ] All distribution strategies work correctly\n- [ ] Merged streams maintain configured ordering\n- [ ] Content-based routing works correctly\n- [ ] Backpressure propagates correctly\n- [ ] No data loss during fan-out/fan-in\n- [ ] Performance scales linearly with branch count\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement stream broadcast",
            "description": "Send each element to all downstream consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nBroadcast sends every element to all consumers.\n\n## Requirements\n\n- Clone elements for each consumer\n- Handle slow consumers\n- Configurable buffer size\n\n## Acceptance Criteria\n\n- [ ] All consumers receive all elements\n- [ ] Slow consumers don't block fast ones\n- [ ] Memory usage is bounded\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement round-robin distribution",
            "description": "Distribute elements evenly across consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nRound-robin load balances across consumers.\n\n## Requirements\n\n- Cycle through consumers in order\n- Handle consumer addition/removal\n- Even distribution over time\n\n## Acceptance Criteria\n\n- [ ] Distribution is even\n- [ ] Dynamic consumer changes handled\n- [ ] No element loss during rebalance\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement content-based routing",
            "description": "Route elements based on content to specific consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nContent-based routing directs elements based on their properties.\n\n## Requirements\n\n- Accept routing function\n- Support default route\n- Handle unroutable elements\n\n## Acceptance Criteria\n\n- [ ] Elements route to correct consumer\n- [ ] Default route catches unmatched\n- [ ] Routing function errors handled\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement stream merge",
            "description": "Combine multiple input streams into one output",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMerge combines multiple streams into a single output.\n\n## Requirements\n\n- Merge with configurable ordering (FIFO, round-robin, priority)\n- Handle stream completion\n- Propagate errors from any source\n\n## Acceptance Criteria\n\n- [ ] All elements from all streams appear in output\n- [ ] Ordering matches configuration\n- [ ] Completed streams don't block others\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 6,
        "title": "Add Data Format Support",
        "description": "Add support for additional data formats including JSON streaming, CSV, Parquet, and more.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nSupporting common data formats makes the framework more useful for real-world data processing tasks.\n\n## Implementation Approach\n\n1. Create format-specific producers and consumers\n2. Add serialization/deserialization transformers\n3. Integrate with serde ecosystem\n4. Support streaming parsing for large files\n\n## Technical Considerations\n\n- Memory efficiency for large files\n- Streaming vs buffered parsing\n- Error handling for malformed data\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] JSON streaming works with large files\n- [ ] CSV parsing handles various dialects\n- [ ] Parquet read/write implemented\n- [ ] All formats integrate with serde\n- [ ] Streaming parsing doesn't load entire file\n- [ ] Malformed data handled gracefully\n- [ ] Performance comparable to dedicated libraries\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JSON streaming",
            "description": "Add JSON Lines producer and consumer with serde integration",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nJSON Lines (JSONL) is a common format for streaming JSON data.\n\n## Requirements\n\n- Parse newline-delimited JSON\n- Integrate with serde for type-safe parsing\n- Stream large files without loading all into memory\n\n## Acceptance Criteria\n\n- [ ] JSONL files parsed correctly\n- [ ] Serde deserialization works\n- [ ] Memory usage constant regardless of file size\n- [ ] Parse errors handled per element\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement CSV support",
            "description": "Add CSV producer and consumer with header handling",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nCSV is ubiquitous for tabular data.\n\n## Requirements\n\n- Parse CSV with configurable delimiter\n- Handle headers and type inference\n- Support quoted fields and escaping\n\n## Acceptance Criteria\n\n- [ ] Various CSV dialects parsed correctly\n- [ ] Headers extracted and used for field names\n- [ ] Streaming parsing for large files\n- [ ] Write support with proper escaping\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement Parquet support",
            "description": "Add Parquet file producer and consumer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nParquet is efficient for analytics workloads.\n\n## Requirements\n\n- Read Parquet files with column selection\n- Write Parquet with compression\n- Support row group streaming\n\n## Acceptance Criteria\n\n- [ ] Parquet files read correctly\n- [ ] Column projection pushdown works\n- [ ] Compression options configurable\n- [ ] Large files don't exhaust memory\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement MessagePack support",
            "description": "Add MessagePack serialization transformer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMessagePack is a compact binary format.\n\n## Requirements\n\n- Serialize to MessagePack\n- Deserialize from MessagePack\n- Integrate with serde\n\n## Acceptance Criteria\n\n- [ ] Round-trip serialization works\n- [ ] Performance better than JSON\n- [ ] Size smaller than JSON equivalent\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Distributed Processing",
        "description": "Support distributed stream processing across multiple nodes for horizontal scalability.",
        "status": "done",
        "priority": "low",
        "dependencies": [
          1,
          2,
          5
        ],
        "details": "## Context\n\nDistributed processing enables horizontal scaling for high-volume streams that exceed single-node capacity.\n\n## Implementation Approach\n\n1. Design coordinator/worker architecture\n2. Implement data partitioning strategies\n3. Add network communication layer\n4. Implement fault tolerance and recovery\n\n## Technical Considerations\n\n- Network partition handling\n- Exactly-once semantics across nodes\n- Shuffle and redistribution costs\n- State management in distributed setting\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Pipelines run across multiple nodes\n- [ ] Data partitioned correctly by key\n- [ ] Worker failures handled with recovery\n- [ ] Coordinator failover works\n- [ ] Network partitions handled gracefully\n- [ ] Performance scales linearly with nodes\n- [ ] Exactly-once preserved across cluster\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design distributed architecture",
            "description": "Create architecture document for distributed processing",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nA solid architecture is essential before implementation.\n\n## Requirements\n\n- Define coordinator and worker roles\n- Specify communication protocol\n- Plan state distribution strategy\n- Document failure modes and recovery\n\n## Acceptance Criteria\n\n- [ ] Architecture document reviewed and approved\n- [ ] All failure modes identified\n- [ ] Protocol defined with message formats\n- [ ] Scalability limits documented\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement data partitioning",
            "description": "Create partitioning strategies for distributing data",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nPartitioning determines how data is distributed across workers.\n\n## Requirements\n\n- Hash-based partitioning by key\n- Range partitioning\n- Custom partitioner support\n\n## Acceptance Criteria\n\n- [ ] Consistent partitioning (same key -> same worker)\n- [ ] Even distribution across workers\n- [ ] Rebalancing on worker change\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement network communication",
            "description": "Add gRPC or custom protocol for inter-node communication",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nEfficient network communication is critical for distributed processing.\n\n## Requirements\n\n- gRPC for RPC communication\n- Streaming data transfer\n- Connection pooling and retry\n\n## Acceptance Criteria\n\n- [ ] Nodes can discover and connect\n- [ ] Data streams efficiently between nodes\n- [ ] Connection failures handled with reconnect\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement fault tolerance",
            "description": "Add worker failure detection and recovery",
            "status": "done",
            "dependencies": [
              2,
              3
            ],
            "details": "## Context\n\nFault tolerance keeps the system running despite failures.\n\n## Requirements\n\n- Heartbeat-based failure detection\n- Work redistribution on failure\n- State recovery from checkpoints\n\n## Acceptance Criteria\n\n- [ ] Failed workers detected within timeout\n- [ ] Work redistributed to healthy workers\n- [ ] No data loss on worker failure\n- [ ] System continues processing after recovery\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 8,
        "title": "WASM Optimizations",
        "description": "Optimize StreamWeave for WebAssembly deployment with browser and server-side WASM support.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nWASM support enables running pipelines in browsers and edge environments. The current codebase supports WASM but needs optimization.\n\n## Implementation Approach\n\n1. Audit current WASM compatibility\n2. Add feature flags for WASM-specific code\n3. Optimize bundle size\n4. Create WASM-specific documentation\n\n## Technical Considerations\n\n- No std library for some targets\n- Memory management differences\n- Async runtime compatibility\n- Bundle size concerns\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] All core features work in WASM\n- [ ] Bundle size under 500KB gzipped\n- [ ] Memory usage acceptable in browser\n- [ ] Examples work in browser and Node.js\n- [ ] Documentation covers WASM deployment\n- [ ] CI tests WASM targets\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit WASM compatibility",
            "description": "Test all features in WASM and document incompatibilities",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nNeed to understand current WASM support level.\n\n## Requirements\n\n- Test each module in wasm32 target\n- Document what works and what doesn't\n- Identify dependencies blocking WASM\n\n## Acceptance Criteria\n\n- [ ] Compatibility matrix documented\n- [ ] Blocking issues identified\n- [ ] Plan for fixes created\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Add WASM feature flags",
            "description": "Create feature flags for WASM-specific code paths",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nFeature flags allow conditional compilation for WASM.\n\n## Requirements\n\n- 'wasm' feature flag in Cargo.toml\n- Conditional compilation for incompatible code\n- WASM-specific implementations where needed\n\n## Acceptance Criteria\n\n- [ ] Feature flag controls WASM-specific code\n- [ ] Native build unaffected by WASM code\n- [ ] Both targets compile cleanly\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Optimize bundle size",
            "description": "Reduce WASM bundle size through optimization",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nSmall bundle size is important for web deployment.\n\n## Requirements\n\n- Enable LTO and size optimization\n- Remove unused code with wasm-opt\n- Optional features for code splitting\n\n## Acceptance Criteria\n\n- [ ] Bundle under 500KB gzipped\n- [ ] Core features available in minimal build\n- [ ] Size regression tests in CI\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Create WASM documentation and examples",
            "description": "Write WASM-specific docs and browser examples",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nUsers need guidance for WASM deployment.\n\n## Requirements\n\n- Getting started guide for WASM\n- Browser example with webpack/vite\n- Node.js example\n- Performance tips\n\n## Acceptance Criteria\n\n- [ ] WASM docs in README or separate file\n- [ ] Working browser example\n- [ ] Working Node.js example\n- [ ] Common issues documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 9,
        "title": "Add Specialized Producers and Consumers",
        "description": "Implement producers and consumers for common external systems like Kafka, Redis, and databases.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nIntegration with external systems makes StreamWeave useful for real-world applications. These are the most commonly requested integrations.\n\n## Implementation Approach\n\n1. Define standard interfaces for external systems\n2. Implement each integration as optional feature\n3. Ensure consistent configuration patterns\n4. Add integration tests with testcontainers\n\n## Technical Considerations\n\n- Connection pooling and management\n- Retry and reconnection logic\n- Backpressure handling\n- Configuration consistency\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Kafka producer/consumer work with real cluster\n- [ ] Redis Streams integration works\n- [ ] Database query producer handles large results\n- [ ] All integrations have retry logic\n- [ ] Configuration is consistent across integrations\n- [ ] Integration tests pass in CI\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Kafka producer/consumer",
            "description": "Add Kafka integration for producing and consuming messages",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nKafka is the most common streaming platform.\n\n## Requirements\n\n- Consume from Kafka topics\n- Produce to Kafka topics\n- Support consumer groups\n- Handle partition assignment\n\n## Acceptance Criteria\n\n- [ ] Can consume from multiple partitions\n- [ ] Offset management works\n- [ ] Producer batching configurable\n- [ ] Connection failures handled\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement Redis Streams producer/consumer",
            "description": "Add Redis Streams integration",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nRedis Streams provides lightweight message streaming.\n\n## Requirements\n\n- XREAD for consuming\n- XADD for producing\n- Consumer group support\n- Message acknowledgment\n\n## Acceptance Criteria\n\n- [ ] Can read from streams\n- [ ] Can write to streams\n- [ ] Consumer groups work\n- [ ] Pending messages tracked\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement database query producer",
            "description": "Add SQL database producer for query results",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDatabase queries are a common data source.\n\n## Requirements\n\n- Support PostgreSQL and MySQL\n- Cursor-based iteration for large results\n- Parameterized queries\n- Connection pooling\n\n## Acceptance Criteria\n\n- [ ] Large query results streamed efficiently\n- [ ] Memory usage bounded\n- [ ] Connection pool managed correctly\n- [ ] Multiple databases supported\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement HTTP polling producer",
            "description": "Add producer that polls HTTP endpoints",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nPolling APIs is common for integrations without webhooks.\n\n## Requirements\n\n- Configurable poll interval\n- Support for pagination\n- Delta detection (only new items)\n- Rate limiting\n\n## Acceptance Criteria\n\n- [ ] Polls at configured interval\n- [ ] Handles pagination correctly\n- [ ] Emits only new items\n- [ ] Rate limits respected\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Comprehensive Examples",
        "description": "Create examples demonstrating all major functionality to help users understand and adopt StreamWeave features.",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "details": "## Context\n\nExamples are essential for helping users understand how to use StreamWeave effectively. While we have basic and advanced pipeline examples, many key features lack dedicated examples showing real-world usage patterns.\n\n## Implementation Approach\n\n1. Create example for each major integration (Kafka, Redis, Database, HTTP)\n2. Add examples for advanced transformers and patterns\n3. Create examples demonstrating error handling and resilience patterns\n4. Update README.md to reflect current capabilities\n\n## Technical Considerations\n\n- Examples should be runnable and well-documented\n- Use realistic scenarios that demonstrate value\n- Include setup instructions for external dependencies\n- Show both simple and complex use cases\n- Each example should have a README with explanation\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Each example compiles and runs successfully\n- [ ] Examples demonstrate real-world usage patterns\n- [ ] All examples have README with setup instructions\n- [ ] README.md accurately reflects current feature set\n- [ ] Examples are listed in main README\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Kafka integration example",
            "description": "Add example demonstrating Kafka producer and consumer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nKafka is one of the most common message streaming platforms. Users need a clear example showing how to consume from and produce to Kafka topics.\n\n## Requirements\n\n- Demonstrate consuming from Kafka topics with consumer groups\n- Show producing messages to Kafka with batching\n- Include setup instructions for local Kafka instance\n- Show offset management and error handling\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] README explains Kafka setup\n- [ ] Demonstrates consumer groups and partition handling\n- [ ] Shows producer batching configuration\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Create Redis Streams integration example",
            "description": "Add example demonstrating Redis Streams producer and consumer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nRedis Streams provides lightweight message streaming. An example helps users understand the integration.\n\n## Requirements\n\n- Demonstrate XREAD and XADD operations\n- Show consumer group usage\n- Include message acknowledgment patterns\n- Show setup for local Redis instance\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] README explains Redis setup\n- [ ] Demonstrates consumer groups\n- [ ] Shows message acknowledgment\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Create database query example",
            "description": "Add example demonstrating database query producer with SQLite",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDatabase queries are a common data source. Users need examples showing how to stream query results efficiently.\n\n## Requirements\n\n- Use in-memory SQLite for easy testing\n- Demonstrate parameterized queries\n- Show cursor-based streaming for large results\n- Include connection pooling configuration\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Uses in-memory SQLite (no external setup needed)\n- [ ] Demonstrates parameterized queries\n- [ ] Shows streaming large result sets\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Create HTTP polling example",
            "description": "Add example demonstrating HTTP polling producer with pagination",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nHTTP polling is common for APIs without webhooks. Users need examples showing polling, pagination, and delta detection.\n\n## Requirements\n\n- Demonstrate polling an HTTP endpoint\n- Show pagination handling (query params, Link headers, or JSON fields)\n- Include delta detection to emit only new items\n- Show rate limiting configuration\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates pagination strategies\n- [ ] Shows delta detection in action\n- [ ] Includes rate limiting\n- [ ] Uses mock HTTP server or public API\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 5,
            "title": "Create file formats example",
            "description": "Add example demonstrating CSV, JSONL, and Parquet producers/consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nFile format support is essential for data processing. Users need examples showing how to read and write various formats.\n\n## Requirements\n\n- Demonstrate CSV read/write with headers\n- Show JSONL streaming for large files\n- Include Parquet read/write with column projection\n- Show streaming parsing to avoid loading entire files\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates CSV with header handling\n- [ ] Shows JSONL streaming\n- [ ] Includes Parquet column projection\n- [ ] Uses test data files in example directory\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 6,
            "title": "Create stateful processing example",
            "description": "Add example demonstrating stateful transformers (RunningSum, MovingAverage)",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nStateful processing enables aggregations and analytics. Users need examples showing how state is maintained across items.\n\n## Requirements\n\n- Demonstrate RunningSumTransformer for cumulative calculations\n- Show MovingAverageTransformer with configurable window size\n- Include state checkpointing example\n- Explain state lifecycle and cleanup\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates RunningSum\n- [ ] Shows MovingAverage with different window sizes\n- [ ] Includes checkpoint example if applicable\n- [ ] Explains state management clearly\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 7,
            "title": "Create error handling example",
            "description": "Add example demonstrating error handling strategies (Stop, Skip, Retry, Custom)",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nError handling is critical for production pipelines. Users need examples showing different strategies and when to use them.\n\n## Requirements\n\n- Demonstrate Stop, Skip, Retry, and Custom error strategies\n- Show pipeline-level and component-level error handling\n- Include examples of error recovery patterns\n- Show how to handle different error types\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates all error handling strategies\n- [ ] Shows pipeline and component-level configuration\n- [ ] Includes realistic error scenarios\n- [ ] Explains when to use each strategy\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 8,
            "title": "Create advanced transformers example",
            "description": "Add example demonstrating CircuitBreaker, Retry, Batch, and RateLimit transformers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nAdvanced transformers provide resilience and performance optimization. Examples help users understand their use cases.\n\n## Requirements\n\n- Demonstrate CircuitBreaker for external service calls\n- Show Retry transformer with exponential backoff\n- Include Batch transformer for grouping items\n- Show RateLimit transformer for throughput control\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates CircuitBreaker with failure scenarios\n- [ ] Shows Retry with backoff strategies\n- [ ] Includes Batch with time and size triggers\n- [ ] Shows RateLimit configuration\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 9,
            "title": "Create windowing operations example",
            "description": "Add example demonstrating time-based and count-based windowing",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nWindowing is fundamental to stream processing. Users need examples showing different window types and use cases.\n\n## Requirements\n\n- Demonstrate tumbling windows (time and count-based)\n- Show sliding windows with overlap\n- Include session windows for gap-based grouping\n- Show watermark handling for event-time processing\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates tumbling windows\n- [ ] Shows sliding windows\n- [ ] Includes session windows\n- [ ] Explains window triggers and emissions\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 10,
            "title": "Create exactly-once processing example",
            "description": "Add example demonstrating message deduplication and exactly-once guarantees",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nExactly-once processing ensures reliable stream processing. Users need examples showing deduplication and idempotency patterns.\n\n## Requirements\n\n- Demonstrate MessageDedupeTransformer\n- Show offset tracking and resume from checkpoint\n- Include transaction example if applicable\n- Show how to handle duplicates in retry scenarios\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates message deduplication\n- [ ] Shows offset tracking and recovery\n- [ ] Includes checkpoint example\n- [ ] Explains exactly-once semantics\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 11,
            "title": "Update README.md with current capabilities",
            "description": "Update README.md to accurately reflect all implemented features and provide links to examples",
            "status": "done",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10
            ],
            "details": "## Context\n\nThe README.md currently lists many features as 'Planned' that are actually implemented. It needs to be updated to reflect the current state and guide users to appropriate examples.\n\n## Requirements\n\n- Move implemented features from 'Planned' to 'Implemented'\n- Add links to all new examples\n- Update feature list to include Kafka, Redis, Database, HTTP polling integrations\n- Add stateful processing, exactly-once, windowing to implemented list\n- Include example links in appropriate sections\n- Update code examples if needed\n\n## Acceptance Criteria\n\n- [ ] All implemented features accurately listed\n- [ ] Planned features list only truly unimplemented items\n- [ ] Examples are linked from relevant sections\n- [ ] Code examples are current and work\n- [ ] README provides clear navigation to examples\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Monitoring and Metrics",
        "description": "Add comprehensive observability with metrics collection, tracing, and monitoring integration.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nObservability is essential for production deployments. Users need to understand pipeline performance and health.\n\n## Implementation Approach\n\n1. Define standard metrics (throughput, latency, errors)\n2. Integrate with OpenTelemetry\n3. Add Prometheus export\n4. Support distributed tracing\n\n## Technical Considerations\n\n- Minimal overhead for metrics collection\n- Cardinality management\n- Trace context propagation\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation\n\n## Example Requirements\n\nAn example should be created demonstrating the monitoring and metrics functionality. The example should show how to:\n- Collect and view standard metrics (throughput, latency, errors)\n- Export metrics to Prometheus\n- Generate OpenTelemetry traces\n- Use health check endpoints\n- Visualize metrics in dashboards",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Throughput, latency, error metrics collected\n- [ ] Metrics exported to Prometheus\n- [ ] OpenTelemetry traces generated\n- [ ] Trace context propagates through pipeline\n- [ ] Metrics overhead under 5%\n- [ ] Health check endpoint available\n- [ ] Dashboard templates provided\n- [ ] Example demonstrating the monitoring and metrics functionality is created and runs successfully\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Define standard metrics",
            "description": "Create standard metrics for pipeline monitoring",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nStandard metrics enable consistent monitoring.\n\n## Requirements\n\n- Throughput (items/second)\n- Latency (p50, p95, p99)\n- Error rate and types\n- Backpressure indicators\n\n## Acceptance Criteria\n\n- [ ] All standard metrics defined\n- [ ] Metrics have appropriate labels\n- [ ] Collection is efficient\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement OpenTelemetry integration",
            "description": "Add OpenTelemetry for traces and metrics",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nOpenTelemetry is the standard for observability.\n\n## Requirements\n\n- Trace spans for pipeline stages\n- Metrics via OTLP\n- Context propagation\n\n## Acceptance Criteria\n\n- [ ] Traces show pipeline execution\n- [ ] Metrics export via OTLP\n- [ ] Context propagates correctly\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Add Prometheus metrics export",
            "description": "Export metrics in Prometheus format",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nPrometheus is widely used for metrics.\n\n## Requirements\n\n- /metrics endpoint\n- Standard Prometheus format\n- Configurable labels\n\n## Acceptance Criteria\n\n- [ ] Metrics endpoint works\n- [ ] Prometheus can scrape metrics\n- [ ] Grafana dashboards work\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement health checks",
            "description": "Add health check endpoint and liveness probes",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nHealth checks enable orchestrator integration.\n\n## Requirements\n\n- /health endpoint\n- Liveness and readiness probes\n- Component health aggregation\n\n## Acceptance Criteria\n\n- [ ] Health endpoint returns status\n- [ ] Unhealthy components detected\n- [ ] Kubernetes probes work\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement SQL-Like Querying",
        "description": "Add SQL-like query interface for intuitive stream processing with SELECT, WHERE, GROUP BY syntax.",
        "status": "done",
        "priority": "low",
        "dependencies": [
          3,
          4
        ],
        "details": "## Context\n\nSQL is familiar to many developers. A SQL-like interface lowers the learning curve for stream processing.\n\n## Implementation Approach\n\n1. Design SQL dialect for streams\n2. Implement SQL parser\n3. Translate SQL to pipeline operations\n4. Add query optimization\n\n## Technical Considerations\n\n- SQL semantics in streaming context\n- Query optimization opportunities\n- Type safety with SQL\n- Error messages for invalid queries\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation\n\n## Example Requirements\n\nAn example should be created demonstrating the SQL-like querying functionality. The example should show how to:\n- Write SQL queries for stream processing\n- Use SELECT, WHERE, GROUP BY, and JOIN operations\n- Execute queries and view results\n- Understand query optimization\n- Handle different data types and aggregations",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] SELECT, WHERE, GROUP BY work\n- [ ] Aggregations produce correct results\n- [ ] JOIN works for windowed streams\n- [ ] Queries compile to efficient pipelines\n- [ ] Type errors caught at parse time\n- [ ] Query optimizer improves performance\n- [ ] Examples show common query patterns\n- [ ] Example demonstrating the SQL-like querying functionality is created and runs successfully\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design SQL dialect",
            "description": "Define the SQL syntax for stream queries",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nNeed to define what SQL constructs are supported.\n\n## Requirements\n\n- SELECT for projection\n- WHERE for filtering\n- GROUP BY for aggregation\n- WINDOW for windowing\n- JOIN for stream joining\n\n## Acceptance Criteria\n\n- [ ] Syntax documented\n- [ ] Semantics clearly defined\n- [ ] Examples for each construct\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement SQL parser",
            "description": "Create parser for the SQL dialect",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nParser converts SQL text to AST.\n\n## Requirements\n\n- Use sqlparser-rs or custom parser\n- Parse all supported constructs\n- Helpful error messages\n\n## Acceptance Criteria\n\n- [ ] All syntax parsed correctly\n- [ ] Errors include position\n- [ ] Parser is well-tested\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement query to pipeline translation",
            "description": "Convert SQL AST to pipeline operations",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nTranslation converts SQL to efficient pipelines.\n\n## Requirements\n\n- Map SQL operations to transformers\n- Handle complex queries with multiple operations\n- Preserve semantics correctly\n\n## Acceptance Criteria\n\n- [ ] Simple queries translate correctly\n- [ ] Complex queries work\n- [ ] Result matches SQL semantics\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Add query optimization",
            "description": "Optimize query plans for better performance",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nOptimization improves query performance.\n\n## Requirements\n\n- Predicate pushdown\n- Projection pruning\n- Join ordering\n\n## Acceptance Criteria\n\n- [ ] Optimizer improves query performance\n- [ ] No semantic changes from optimization\n- [ ] Optimization rules are tested\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Machine Learning Integration",
        "description": "Add ML pipeline support with model inference, feature extraction, and integration with ML frameworks.",
        "status": "pending",
        "priority": "low",
        "dependencies": [
          1,
          4
        ],
        "details": "## Context\n\nML inference on streams enables real-time predictions. This is increasingly common in modern applications.\n\n## Implementation Approach\n\n1. Design inference transformer interface\n2. Integrate with ONNX Runtime\n3. Add batching for efficient inference\n4. Support model hot-swapping\n\n## Technical Considerations\n\n- Inference latency requirements\n- Batch size optimization\n- Model loading and memory\n- GPU support considerations\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation\n\n## Example Requirements\n\nAn example should be created demonstrating the machine learning integration functionality. The example should show how to:\n- Load and run ONNX models in pipelines\n- Use batching for efficient inference\n- Perform feature extraction\n- Hot-swap models at runtime\n- Build end-to-end ML pipelines with real-time predictions",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] ONNX models load and run inference\n- [ ] Batching improves throughput\n- [ ] Model can be swapped without restart\n- [ ] Feature extraction utilities work\n- [ ] Latency acceptable for streaming use\n- [ ] Examples show end-to-end ML pipeline\n- [ ] Example demonstrating the machine learning integration functionality is created and runs successfully\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design inference transformer interface",
            "description": "Create trait for model inference transformers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nA common interface enables different ML backends.\n\n## Requirements\n\n- Load model from path/bytes\n- Run inference on input\n- Support batched inference\n- Handle model errors\n\n## Acceptance Criteria\n\n- [ ] Trait is flexible for different frameworks\n- [ ] Batching is supported\n- [ ] Errors handled gracefully\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement ONNX Runtime integration",
            "description": "Add ONNX Runtime backend for inference",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nONNX is a portable model format.\n\n## Requirements\n\n- Load ONNX models\n- Run inference with ONNX Runtime\n- Support CPU and GPU execution\n\n## Acceptance Criteria\n\n- [ ] ONNX models load correctly\n- [ ] Inference produces correct results\n- [ ] GPU execution works if available\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement inference batching",
            "description": "Add batching for efficient model inference",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nBatching improves GPU utilization.\n\n## Requirements\n\n- Configurable batch size\n- Timeout for partial batches\n- Dynamic batching based on load\n\n## Acceptance Criteria\n\n- [ ] Batching improves throughput\n- [ ] Timeouts prevent stale data\n- [ ] Latency impact acceptable\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement model hot-swapping",
            "description": "Allow model replacement without pipeline restart",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nHot-swapping enables model updates without downtime.\n\n## Requirements\n\n- Load new model while old is serving\n- Atomic swap to new model\n- Graceful handling of in-flight requests\n\n## Acceptance Criteria\n\n- [ ] Model can be swapped at runtime\n- [ ] No requests lost during swap\n- [ ] Swap is atomic\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 14,
        "title": "Add Visualization Tools",
        "description": "Create tools for pipeline visualization, debugging, and interactive development.",
        "status": "pending",
        "priority": "low",
        "dependencies": [
          11
        ],
        "details": "## Context\n\nVisualization helps users understand and debug pipelines. Interactive tools improve developer experience.\n\n## Implementation Approach\n\n1. Generate pipeline DAG representation\n2. Create web-based visualization\n3. Add real-time data flow display\n4. Implement debug mode\n\n## Technical Considerations\n\n- Web UI technology choices\n- Real-time updates via WebSocket\n- Performance impact of visualization\n- Integration with IDE\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation\n\n## Example Requirements\n\nAn example should be created demonstrating the visualization tools functionality. The example should show how to:\n- Generate and view pipeline DAG representations\n- Use the web-based visualization UI\n- Monitor real-time data flow through pipelines\n- Use debug mode for step-through debugging\n- View performance profiling and metrics",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Pipeline DAG visualized correctly\n- [ ] Real-time data flow shown\n- [ ] Debug mode allows step-through\n- [ ] Performance profiling visualized\n- [ ] UI works in major browsers\n- [ ] Low overhead when not visualizing\n- [ ] Example demonstrating the visualization tools functionality is created and runs successfully\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Generate pipeline DAG representation",
            "description": "Create data structure representing pipeline as DAG",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDAG representation enables visualization.\n\n## Requirements\n\n- Capture all nodes and edges\n- Include node metadata\n- Export to common formats (DOT, JSON)\n\n## Acceptance Criteria\n\n- [ ] DAG captures pipeline structure\n- [ ] Metadata includes types, config\n- [ ] Export formats work\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Create web visualization UI",
            "description": "Build web-based pipeline visualization",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nWeb UI allows interactive exploration.\n\n## Requirements\n\n- Display pipeline DAG\n- Zoom and pan\n- Node details on click\n\n## Acceptance Criteria\n\n- [ ] Pipeline displayed clearly\n- [ ] Navigation is smooth\n- [ ] Node info accessible\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Add real-time data flow display",
            "description": "Show data flowing through pipeline in real-time",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nReal-time display helps understand pipeline behavior.\n\n## Requirements\n\n- Show throughput per node\n- Animate data flow\n- Highlight bottlenecks\n\n## Acceptance Criteria\n\n- [ ] Throughput displayed accurately\n- [ ] Animation is smooth\n- [ ] Bottlenecks visible\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Implement debug mode",
            "description": "Add step-through debugging for pipelines",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nDebug mode helps troubleshoot issues.\n\n## Requirements\n\n- Pause at breakpoints\n- Inspect data at each stage\n- Step forward/backward\n\n## Acceptance Criteria\n\n- [ ] Breakpoints work\n- [ ] Data inspection works\n- [ ] Stepping is reliable\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      },
      {
        "id": 16,
        "title": "Implement HTTP Server Support",
        "description": "Add HTTP server capabilities enabling REST microservices with streaming request/response bodies through StreamWeave pipelines.",
        "status": "pending",
        "priority": "high",
        "dependencies": [],
        "details": "## Context\n\nCurrently, StreamWeave can only act as an HTTP client (via HttpPollProducer). To enable building complete REST microservices, we need HTTP server capabilities that allow:\n- Streaming HTTP requests through pipelines\n- Streaming HTTP request bodies as stream items\n- Streaming HTTP response bodies from pipeline outputs\n- Integration with Axum for route handling and middleware\n- Support for REST API patterns (GET, POST, PUT, DELETE, etc.)\n\n## Implementation Approach\n\n1. Add Axum dependency as optional feature\n2. Create HTTP request producer that converts incoming HTTP requests to stream items\n3. Create HTTP response consumer that converts stream items to HTTP responses\n4. Support streaming request/response bodies\n5. Integrate with Axum router for route handling\n6. Add middleware support for authentication, CORS, etc.\n\n## Technical Considerations\n\n- Type safety between request/response types and pipeline types\n- Streaming large request/response bodies efficiently\n- Error handling and HTTP status codes\n- Async request handling with Tokio\n- Integration with existing error handling strategies\n- Support for different content types (JSON, text, binary)\n- Connection pooling and concurrent request handling\n- WASM compatibility considerations (may need feature flags)\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation\n\n## Example Requirements\n\nAn example should be created demonstrating the HTTP server support functionality. The example should show how to:\n- Set up an HTTP server with Axum integration\n- Convert HTTP requests to stream items\n- Process requests through StreamWeave pipelines\n- Convert pipeline outputs to HTTP responses\n- Handle streaming request and response bodies\n- Apply middleware (CORS, authentication, logging)\n- Handle errors and return appropriate status codes\n- Build a complete REST microservice\n\n## Comprehensive Catalog of Reusable Pipeline Components\n\nThis section catalogs ALL reusable pipeline components that could be implemented across the StreamWeave ecosystem, organized by category. This serves as a comprehensive reference for future development.\n\n### PRODUCERS (Data Sources)\n\n#### Core/Collection Producers\n-  **ArrayProducer** - Produces items from an array/vector\n-  **VecProducer** - Produces items from a vector\n-  **RangeProducer** - Produces numeric ranges (start, end, step)\n-  **StringProducer** - Produces items from string slices or lines\n-  **HashMapProducer** - Produces key-value pairs from HashMap\n-  **HashSetProducer** - Produces items from HashSet\n-  **ChannelProducer** - Produces items from async channels (tokio::sync::mpsc)\n-  **IntervalProducer** - Produces items at regular time intervals\n-  **RandomNumberProducer** - Produces random numbers (requires random feature)\n\n#### File System Producers\n-  **FileProducer** - Reads data from files line-by-line or in chunks\n-  **CsvProducer** - Reads CSV files with header support (requires file-formats feature)\n-  **JsonlProducer** - Reads JSON Lines files (requires file-formats feature)\n-  **MsgPackProducer** - Reads MessagePack files (requires file-formats feature)\n-  **ParquetProducer** - Reads Parquet files with column projection (requires file-formats feature)\n-  **DirectoryProducer** - Produces file paths from directory traversal\n-  **GlobProducer** - Produces file paths matching glob patterns\n-  **WatchProducer** - Produces file system events (file created, modified, deleted)\n-  **TailProducer** - Produces new lines from files (like `tail -f`)\n-  **ArchiveProducer** - Produces entries from ZIP/TAR archives\n-  **XmlProducer** - Reads XML files with streaming parser\n-  **YamlProducer** - Reads YAML files with streaming parser\n-  **TomlProducer** - Reads TOML files\n-  **AvroProducer** - Reads Apache Avro files\n-  **OrcProducer** - Reads Apache ORC files\n\n#### Network/Protocol Producers\n-  **HttpPollProducer** - Polls HTTP endpoints with pagination support (requires http-poll feature)\n-  **KafkaProducer** - Consumes from Kafka topics with consumer groups (requires kafka feature)\n-  **RedisStreamsProducer** - Consumes from Redis Streams (requires redis-streams feature)\n-  **HttpServerProducer** - Produces HTTP requests from incoming server requests (this task)\n-  **WebSocketProducer** - Produces messages from WebSocket connections\n-  **ServerSentEventsProducer** - Produces events from SSE streams\n-  **GrpcProducer** - Produces items from gRPC streaming calls\n-  **TcpProducer** - Produces data from TCP connections\n-  **UdpProducer** - Produces datagrams from UDP sockets\n-  **UnixSocketProducer** - Produces data from Unix domain sockets\n-  **MqttProducer** - Consumes from MQTT topics\n-  **RabbitMqProducer** - Consumes from RabbitMQ queues\n-  **NatsProducer** - Consumes from NATS subjects\n-  **PulsarProducer** - Consumes from Apache Pulsar topics\n-  **AmazonSqsProducer** - Consumes from AWS SQS queues\n-  **AmazonKinesisProducer** - Consumes from AWS Kinesis streams\n-  **GooglePubSubProducer** - Consumes from Google Cloud Pub/Sub\n-  **AzureServiceBusProducer** - Consumes from Azure Service Bus\n-  **FtpProducer** - Produces files from FTP servers\n-  **SftpProducer** - Produces files from SFTP servers\n-  **S3Producer** - Produces objects from AWS S3 buckets\n-  **GcsProducer** - Produces objects from Google Cloud Storage\n-  **AzureBlobProducer** - Produces blobs from Azure Blob Storage\n\n#### Database Producers\n-  **DatabaseProducer** - Queries databases (PostgreSQL, MySQL, SQLite) with sqlx (requires database feature)\n-  **MongoDbProducer** - Produces documents from MongoDB collections\n-  **CassandraProducer** - Produces rows from Cassandra tables\n-  **DynamoDbProducer** - Produces items from AWS DynamoDB tables\n-  **ElasticsearchProducer** - Produces documents from Elasticsearch queries\n-  **InfluxDbProducer** - Produces time-series data from InfluxDB\n-  **TimescaleDbProducer** - Produces time-series data from TimescaleDB\n-  **ClickHouseProducer** - Produces rows from ClickHouse queries\n-  **BigQueryProducer** - Produces rows from Google BigQuery\n-  **RedshiftProducer** - Produces rows from AWS Redshift\n-  **SnowflakeProducer** - Produces rows from Snowflake\n-  **ChangeDataCaptureProducer** - Produces database change events (CDC)\n-  **ReplicationLogProducer** - Produces database replication log entries\n\n#### System/OS Producers\n-  **CommandProducer** - Produces output from command execution (native only)\n-  **EnvVarProducer** - Produces environment variables (native only)\n-  **ProcessProducer** - Produces output from running processes\n-  **LogFileProducer** - Produces log entries from system log files\n-  **SyslogProducer** - Produces syslog messages\n-  **MetricsProducer** - Produces system metrics (CPU, memory, disk, network)\n-  **EventProducer** - Produces OS events (file system, network, process)\n-  **SignalProducer** - Produces Unix signals\n-  **TimerProducer** - Produces items at scheduled times (cron-like)\n\n#### Data Generation Producers\n-  **GeneratorProducer** - Produces items from custom generator functions\n-  **SequenceProducer** - Produces sequential IDs or values\n-  **FakeDataProducer** - Produces fake/synthetic data for testing\n-  **TimeSeriesProducer** - Produces time-series data with configurable patterns\n-  **EventGeneratorProducer** - Produces synthetic events for testing\n\n#### Message Queue/Streaming Platform Producers\n-  **AmazonEventBridgeProducer** - Consumes from AWS EventBridge\n-  **GoogleEventarcProducer** - Consumes from Google Eventarc\n-  **AzureEventGridProducer** - Consumes from Azure Event Grid\n-  **ConfluentCloudProducer** - Consumes from Confluent Cloud (Kafka)\n\n#### Specialized Producers\n-  **IteratorProducer** - Produces items from Rust iterators\n-  **StreamProducer** - Produces items from existing futures::Stream\n-  **FutureProducer** - Produces single item from Future\n-  **LazyProducer** - Produces items on-demand with lazy evaluation\n-  **CacheProducer** - Produces items from cache with TTL support\n-  **QueueProducer** - Produces items from in-memory queues\n-  **BufferProducer** - Produces items from buffered sources\n\n### TRANSFORMERS (Data Processing)\n\n#### Core Functional Transformers\n-  **MapTransformer** - Transforms each item with a function\n-  **FilterTransformer** - Filters items based on predicate\n-  **FlatMapTransformer** - Maps and flattens nested structures\n-  **FlattenTransformer** - Flattens nested collections\n-  **ReduceTransformer** - Reduces stream to single value\n-  **DistinctTransformer** - Removes duplicate items\n-  **DedupeTransformer** - Deduplicates items based on key\n-  **MessageDedupeTransformer** - Deduplicates messages by ID with time windows\n-  **SortTransformer** - Sorts items\n-  **GroupByTransformer** - Groups items by key\n-  **JoinTransformer** - Joins two streams on key\n-  **ConcatTransformer** - Concatenates multiple streams\n-  **MergeTransformer** - Merges multiple streams\n-  **OrderedMergeTransformer** - Merges streams maintaining order\n-  **InterleaveTransformer** - Interleaves items from multiple streams\n-  **ZipTransformer** - Zips items from multiple streams\n-  **SplitTransformer** - Splits items into multiple outputs\n-  **SplitAtTransformer** - Splits stream at index\n-  **TakeTransformer** - Takes first N items\n-  **SkipTransformer** - Skips first N items\n-  **LimitTransformer** - Limits stream to N items\n-  **ChunkTransformer** - Groups items into chunks\n-  **BatchTransformer** - Batches items by size or time\n-  **WindowTransformer** - Creates time or count-based windows\n-  **PartitionTransformer** - Partitions items by key\n-  **RouterTransformer** - Routes items to different outputs\n-  **RoundRobinTransformer** - Distributes items round-robin\n-  **BroadcastTransformer** - Broadcasts items to all outputs\n-  **SampleTransformer** - Samples items with probability (requires random feature)\n\n#### Stateful Transformers\n-  **RunningSumTransformer** - Maintains running sum across items\n-  **MovingAverageTransformer** - Calculates moving average with window\n-  **StatefulMapTransformer** - Map with state maintained across items\n-  **StatefulFilterTransformer** - Filter with state\n-  **StatefulReduceTransformer** - Reduce with persistent state\n-  **AccumulatorTransformer** - Accumulates values with custom logic\n-  **StateMachineTransformer** - State machine-based transformation\n-  **SessionTransformer** - Groups items into sessions by gap detection\n\n#### Time-Based Transformers\n-  **DelayTransformer** - Delays items by duration\n-  **DebounceTransformer** - Debounces items by time window\n-  **TimeoutTransformer** - Times out after duration\n-  **ThrottleTransformer** - Throttles items by rate\n-  **TumblingWindowTransformer** - Fixed-size non-overlapping windows\n-  **SlidingWindowTransformer** - Overlapping windows\n-  **SessionWindowTransformer** - Windows based on activity gaps\n-  **WatermarkTransformer** - Adds watermarks for event-time processing\n-  **TimestampTransformer** - Adds timestamps to items\n-  **TimeExtractTransformer** - Extracts time components from timestamps\n\n#### Rate Control & Backpressure Transformers\n-  **RateLimitTransformer** - Limits throughput (items per second)\n-  **BackpressureTransformer** - Handles backpressure with buffering\n-  **BufferTransformer** - Buffers items with size limits\n-  **CircuitBreakerTransformer** - Circuit breaker pattern for external calls\n-  **RetryTransformer** - Retries failed items with backoff\n-  **AdaptiveRateLimitTransformer** - Adapts rate based on downstream capacity\n-  **TokenBucketTransformer** - Token bucket rate limiting\n-  **LeakyBucketTransformer** - Leaky bucket rate limiting\n-  **PriorityTransformer** - Prioritizes items based on priority\n-  **FairTransformer** - Fair scheduling across multiple inputs\n\n#### Data Format Transformers\n-  **JsonParseTransformer** - Parses JSON strings to objects\n-  **JsonStringifyTransformer** - Stringifies objects to JSON\n-  **XmlParseTransformer** - Parses XML to structured data\n-  **XmlSerializeTransformer** - Serializes data to XML\n-  **CsvParseTransformer** - Parses CSV strings to records\n-  **CsvSerializeTransformer** - Serializes records to CSV\n-  **Base64EncodeTransformer** - Base64 encodes data\n-  **Base64DecodeTransformer** - Base64 decodes data\n-  **UrlEncodeTransformer** - URL encodes strings\n-  **UrlDecodeTransformer** - URL decodes strings\n-  **CompressTransformer** - Compresses data (gzip, deflate, etc.)\n-  **DecompressTransformer** - Decompresses data\n-  **EncryptTransformer** - Encrypts data\n-  **DecryptTransformer** - Decrypts data\n-  **HashTransformer** - Computes hashes (MD5, SHA256, etc.)\n-  **SignTransformer** - Signs data with cryptographic signatures\n-  **VerifyTransformer** - Verifies cryptographic signatures\n\n#### Validation & Schema Transformers\n-  **ValidateTransformer** - Validates items against schema\n-  **JsonSchemaTransformer** - Validates JSON against JSON Schema\n-  **TypeCheckTransformer** - Type checks items\n-  **SanitizeTransformer** - Sanitizes data (XSS, SQL injection prevention)\n-  **NormalizeTransformer** - Normalizes data formats\n-  **EnrichTransformer** - Enriches items with additional data\n-  **MaskTransformer** - Masks sensitive data (PII, etc.)\n\n#### Aggregation Transformers\n-  **CountTransformer** - Counts items\n-  **SumTransformer** - Sums numeric values\n-  **AverageTransformer** - Calculates average\n-  **MinTransformer** - Finds minimum value\n-  **MaxTransformer** - Finds maximum value\n-  **MedianTransformer** - Calculates median\n-  **PercentileTransformer** - Calculates percentiles\n-  **VarianceTransformer** - Calculates variance\n-  **StdDevTransformer** - Calculates standard deviation\n-  **HistogramTransformer** - Creates histograms\n-  **TopKTransformer** - Finds top K items\n-  **BottomKTransformer** - Finds bottom K items\n\n#### Machine Learning Transformers\n-  **InferenceTransformer** - Runs ML model inference (requires ml feature)\n-  **BatchedInferenceTransformer** - Batched ML inference for efficiency\n-  **HotswapTransformer** - Hot-swaps ML models without downtime\n-  **FeatureExtractionTransformer** - Extracts features for ML\n-  **PreprocessingTransformer** - Preprocesses data for ML models\n-  **PostprocessingTransformer** - Postprocesses ML model outputs\n-  **EnsembleTransformer** - Combines multiple model predictions\n-  **AnomalyDetectionTransformer** - Detects anomalies in streams\n-  **ClassificationTransformer** - Classifies items\n-  **RegressionTransformer** - Performs regression predictions\n\n#### Data Transformation Transformers\n-  **ReshapeTransformer** - Reshapes data structures\n-  **PivotTransformer** - Pivots data tables\n-  **UnpivotTransformer** - Unpivots data tables\n-  **TransposeTransformer** - Transposes matrices\n-  **NormalizeTransformer** - Normalizes numeric data (min-max, z-score)\n-  **ScaleTransformer** - Scales numeric data\n-  **TransformTransformer** - Applies mathematical transformations\n-  **InterpolateTransformer** - Interpolates missing values\n-  **FillTransformer** - Fills missing values with defaults\n-  **DropTransformer** - Drops null/missing values\n\n#### Routing & Distribution Transformers\n-  **LoadBalanceTransformer** - Load balances across outputs\n-  **ShardTransformer** - Shards items by key\n-  **ReplicateTransformer** - Replicates items to multiple outputs\n-  **FanOutTransformer** - Fans out to multiple pipelines\n-  **FanInTransformer** - Fans in from multiple pipelines\n-  **ConditionalTransformer** - Routes based on conditions\n-  **SwitchTransformer** - Switches between outputs based on value\n\n#### Error Handling Transformers\n-  **ErrorMapTransformer** - Maps errors to different types\n-  **ErrorRecoverTransformer** - Recovers from errors with fallback\n-  **ErrorLogTransformer** - Logs errors without stopping\n-  **ErrorRetryTransformer** - Retries with exponential backoff\n-  **DeadLetterTransformer** - Routes failed items to dead letter queue\n\n#### Monitoring & Observability Transformers\n-  **MetricsTransformer** - Collects metrics on items\n-  **TraceTransformer** - Adds tracing spans\n-  **LogTransformer** - Logs items with configurable levels\n-  **AuditTransformer** - Creates audit logs\n-  **TelemetryTransformer** - Adds telemetry data\n\n#### Utility Transformers\n-  **IdentityTransformer** - Passes items through unchanged (no-op)\n-  **TapTransformer** - Taps into stream for side effects\n-  **PeekTransformer** - Peeks at items without consuming\n-  **DebugTransformer** - Debugs stream with breakpoints\n-  **InspectTransformer** - Inspects items for debugging\n-  **CountTransformer** - Counts items passing through\n-  **StatsTransformer** - Collects statistics on stream\n\n### CONSUMERS (Data Sinks)\n\n#### Core/Collection Consumers\n-  **VecConsumer** - Collects items into vector\n-  **ArrayConsumer** - Collects items into array\n-  **StringConsumer** - Collects items into string\n-  **HashMapConsumer** - Collects key-value pairs into HashMap\n-  **HashSetConsumer** - Collects items into HashSet\n-  **ChannelConsumer** - Sends items to async channels\n\n#### File System Consumers\n-  **FileConsumer** - Writes items to files\n-  **CsvConsumer** - Writes CSV files with headers (requires file-formats feature)\n-  **JsonlConsumer** - Writes JSON Lines files (requires file-formats feature)\n-  **MsgPackConsumer** - Writes MessagePack files (requires file-formats feature)\n-  **ParquetConsumer** - Writes Parquet files (requires file-formats feature)\n-  **DirectoryConsumer** - Writes files to directories with naming\n-  **RotatingFileConsumer** - Writes to rotating log files\n-  **CompressedFileConsumer** - Writes compressed files (gzip, etc.)\n-  **ArchiveConsumer** - Writes items to ZIP/TAR archives\n-  **XmlConsumer** - Writes XML files\n-  **YamlConsumer** - Writes YAML files\n-  **TomlConsumer** - Writes TOML files\n-  **AvroConsumer** - Writes Apache Avro files\n-  **OrcConsumer** - Writes Apache ORC files\n\n#### Network/Protocol Consumers\n-  **KafkaConsumer** - Produces to Kafka topics (requires kafka feature)\n-  **RedisStreamsConsumer** - Produces to Redis Streams (requires redis-streams feature)\n-  **HttpServerConsumer** - Sends HTTP responses (this task)\n-  **HttpClientConsumer** - Sends HTTP requests (POST, PUT, etc.)\n-  **WebSocketConsumer** - Sends messages via WebSocket\n-  **ServerSentEventsConsumer** - Sends Server-Sent Events\n-  **GrpcConsumer** - Sends gRPC streaming responses\n-  **TcpConsumer** - Sends data via TCP connections\n-  **UdpConsumer** - Sends datagrams via UDP\n-  **UnixSocketConsumer** - Sends data via Unix domain sockets\n-  **MqttConsumer** - Publishes to MQTT topics\n-  **RabbitMqConsumer** - Publishes to RabbitMQ queues\n-  **NatsConsumer** - Publishes to NATS subjects\n-  **PulsarConsumer** - Publishes to Apache Pulsar topics\n-  **AmazonSqsConsumer** - Publishes to AWS SQS queues\n-  **AmazonKinesisConsumer** - Publishes to AWS Kinesis streams\n-  **GooglePubSubConsumer** - Publishes to Google Cloud Pub/Sub\n-  **AzureServiceBusConsumer** - Publishes to Azure Service Bus\n-  **FtpConsumer** - Uploads files to FTP servers\n-  **SftpConsumer** - Uploads files to SFTP servers\n-  **S3Consumer** - Uploads objects to AWS S3\n-  **GcsConsumer** - Uploads objects to Google Cloud Storage\n-  **AzureBlobConsumer** - Uploads blobs to Azure Blob Storage\n\n#### Database Consumers\n-  **DatabaseConsumer** - Writes to databases (INSERT, UPDATE, UPSERT)\n-  **MongoDbConsumer** - Writes documents to MongoDB\n-  **CassandraConsumer** - Writes rows to Cassandra\n-  **DynamoDbConsumer** - Writes items to AWS DynamoDB\n-  **ElasticsearchConsumer** - Indexes documents in Elasticsearch\n-  **InfluxDbConsumer** - Writes time-series data to InfluxDB\n-  **TimescaleDbConsumer** - Writes time-series data to TimescaleDB\n-  **ClickHouseConsumer** - Writes rows to ClickHouse\n-  **BigQueryConsumer** - Writes rows to Google BigQuery\n-  **RedshiftConsumer** - Writes rows to AWS Redshift\n-  **SnowflakeConsumer** - Writes rows to Snowflake\n\n#### System/OS Consumers\n-  **ConsoleConsumer** - Prints items to console/stdout (native only)\n-  **CommandConsumer** - Executes commands with items as input (native only)\n-  **LogConsumer** - Writes to system logs\n-  **SyslogConsumer** - Sends syslog messages\n-  **MetricsConsumer** - Exports metrics to monitoring systems\n-  **AlertConsumer** - Sends alerts/notifications\n-  **EmailConsumer** - Sends emails\n-  **SmsConsumer** - Sends SMS messages\n-  **SlackConsumer** - Sends messages to Slack\n-  **DiscordConsumer** - Sends messages to Discord\n-  **TeamsConsumer** - Sends messages to Microsoft Teams\n\n#### Monitoring & Observability Consumers\n-  **PrometheusConsumer** - Exports metrics to Prometheus\n-  **GrafanaConsumer** - Sends data to Grafana\n-  **DatadogConsumer** - Sends metrics/logs to Datadog\n-  **NewRelicConsumer** - Sends data to New Relic\n-  **SplunkConsumer** - Sends logs to Splunk\n-  **ElasticsearchLogConsumer** - Sends logs to Elasticsearch\n-  **CloudWatchConsumer** - Sends logs/metrics to AWS CloudWatch\n-  **StackdriverConsumer** - Sends logs/metrics to Google Stackdriver\n-  **AzureMonitorConsumer** - Sends logs/metrics to Azure Monitor\n-  **OpenTelemetryConsumer** - Exports to OpenTelemetry\n-  **JaegerConsumer** - Sends traces to Jaeger\n-  **ZipkinConsumer** - Sends traces to Zipkin\n\n#### Cache Consumers\n-  **RedisConsumer** - Writes to Redis (strings, hashes, lists, sets)\n-  **MemcachedConsumer** - Writes to Memcached\n-  **InMemoryCacheConsumer** - Writes to in-memory cache\n-  **LruCacheConsumer** - Writes to LRU cache\n\n#### Specialized Consumers\n-  **NullConsumer** - Discards items (sink)\n-  **CountConsumer** - Counts items without storing\n-  **StatsConsumer** - Collects statistics\n-  **ValidatorConsumer** - Validates items and reports errors\n-  **SamplerConsumer** - Samples items for analysis\n-  **BufferConsumer** - Buffers items with size limits\n-  **BatchConsumer** - Batches items before writing\n-  **AsyncConsumer** - Asynchronously processes items\n-  **ParallelConsumer** - Processes items in parallel\n\n### Component Implementation Patterns\n\n#### Common Patterns for New Components\n1. **Trait Implementation**: Implement Producer/Transformer/Consumer trait\n2. **Configuration**: Use Config types (ProducerConfig, TransformerConfig, ConsumerConfig)\n3. **Error Handling**: Integrate with ErrorStrategy system\n4. **Streaming**: Use futures::Stream for async processing\n5. **Type Safety**: Leverage Rust's type system for compile-time guarantees\n6. **Documentation**: Comprehensive doc comments with examples\n7. **Testing**: Unit tests with >90% coverage\n8. **Feature Flags**: Use feature flags for optional dependencies\n9. **WASM Compatibility**: Consider WASM limitations for browser support\n10. **Resource Cleanup**: Implement Drop for proper resource management\n\n#### Integration Patterns\n- **Database Integration**: Use connection pooling, prepared statements, streaming cursors\n- **Message Queue Integration**: Support consumer groups, offset management, acknowledgments\n- **File Format Integration**: Streaming parsers/serializers, schema support, compression\n- **Network Integration**: Connection pooling, retry logic, timeout handling\n- **Cloud Integration**: Authentication, region support, retry with backoff\n\n#### Performance Considerations\n- **Streaming**: Process items as they arrive, don't buffer entire datasets\n- **Batching**: Batch operations for efficiency (database writes, network calls)\n- **Backpressure**: Handle backpressure gracefully\n- **Memory**: Minimize allocations in hot paths\n- **Concurrency**: Support concurrent processing where appropriate\n\nThis comprehensive catalog should guide future component development and help identify gaps in the StreamWeave ecosystem.",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] HTTP requests can be converted to stream items via producer\n- [ ] Stream items can be converted to HTTP responses via consumer\n- [ ] Request bodies can be streamed through pipelines\n- [ ] Response bodies can be streamed from pipeline outputs\n- [ ] Axum integration works for route handling\n- [ ] Multiple HTTP methods supported (GET, POST, PUT, DELETE, PATCH)\n- [ ] Status codes and headers are correctly handled\n- [ ] Error responses work with proper status codes\n- [ ] Middleware can be applied (CORS, auth, logging)\n- [ ] Concurrent requests are handled correctly\n- [ ] Large request/response bodies stream efficiently\n- [ ] Example REST microservice compiles and runs\n- [ ] Example demonstrating the HTTP server support functionality is created and runs successfully\n- [ ] Unit tests achieve >90% coverage\n- [ ] Integration tests verify end-to-end request/response flow\n- [ ] Documentation includes examples for common REST patterns\n- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Axum dependency and HTTP server types",
            "description": "Add Axum as optional dependency and create core HTTP server types for requests and responses",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nWe need Axum as the HTTP server framework and core types to represent HTTP requests and responses in the StreamWeave ecosystem.\n\n## Requirements\n\n- Add `axum` as optional dependency with feature flag `http-server`\n- Create `HttpRequest` type wrapping Axum's request with metadata\n- Create `HttpResponse` type for response data\n- Support common content types (JSON, text/plain, application/octet-stream)\n- Include request metadata (method, path, headers, query params)\n- Include response metadata (status code, headers)\n\n## Acceptance Criteria\n\n- [ ] Axum dependency added with proper feature flag\n- [ ] `HttpRequest` type defined with all request metadata\n- [ ] `HttpResponse` type defined with status and headers\n- [ ] Content type handling for common formats\n- [ ] Types are Send + Sync for async use\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 2,
            "title": "Implement HTTP request producer",
            "description": "Create producer that converts incoming HTTP requests into stream items",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nThe HTTP request producer should accept HTTP requests (likely from an Axum handler) and convert them into stream items that can flow through a StreamWeave pipeline.\n\n## Requirements\n\n- Implement `Producer` trait for HTTP requests\n- Support streaming request bodies (for large payloads)\n- Extract request metadata (headers, query params, path params)\n- Handle different HTTP methods\n- Support both JSON and raw body extraction\n- Handle request errors gracefully\n- Support concurrent requests\n\n## Acceptance Criteria\n\n- [ ] `HttpRequestProducer` implements `Producer<HttpRequest>`\n- [ ] Request bodies can be extracted as JSON or raw bytes\n- [ ] Request metadata (method, path, headers) accessible\n- [ ] Query parameters and path parameters extracted\n- [ ] Concurrent requests handled correctly\n- [ ] Large request bodies stream efficiently\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 3,
            "title": "Implement HTTP response consumer",
            "description": "Create consumer that converts stream items into HTTP responses",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nThe HTTP response consumer should take stream items from a pipeline and convert them into HTTP responses that can be sent back to clients via Axum.\n\n## Requirements\n\n- Implement `Consumer` trait for HTTP responses\n- Support streaming response bodies\n- Set HTTP status codes and headers\n- Handle different content types (JSON, text, binary)\n- Support both single-item and stream responses\n- Handle response errors with appropriate status codes\n- Support chunked transfer encoding for large responses\n\n## Acceptance Criteria\n\n- [ ] `HttpResponseConsumer` implements `Consumer<HttpResponse>`\n- [ ] Response status codes can be set\n- [ ] Response headers can be set\n- [ ] Response bodies can be JSON, text, or binary\n- [ ] Streaming responses work correctly\n- [ ] Error responses use appropriate status codes\n- [ ] Large responses stream efficiently\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 4,
            "title": "Add Axum route handler integration",
            "description": "Create Axum handlers that integrate HTTP requests/responses with StreamWeave pipelines",
            "status": "done",
            "dependencies": [
              2,
              3
            ],
            "details": "## Context\n\nWe need Axum route handlers that bridge the gap between HTTP requests/responses and StreamWeave pipelines, allowing users to define REST endpoints using StreamWeave for processing.\n\n## Requirements\n\n- Create handler function that accepts pipeline configuration\n- Convert Axum request to HttpRequest\n- Run pipeline with request as input\n- Convert pipeline output to Axum response\n- Support async request handling\n- Handle pipeline errors and convert to HTTP errors\n- Support multiple HTTP methods per route\n\n## Acceptance Criteria\n\n- [ ] Handler function works with Axum router\n- [ ] Requests flow through pipeline correctly\n- [ ] Pipeline outputs converted to responses\n- [ ] Errors from pipeline result in appropriate HTTP errors\n- [ ] Async handling works correctly\n- [ ] Multiple routes can be defined\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 5,
            "title": "Support streaming request bodies",
            "description": "Enable streaming large HTTP request bodies through pipelines as stream items",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nFor large request bodies (e.g., file uploads, large JSON arrays), we should support streaming the body content through the pipeline rather than loading it all into memory.\n\n## Requirements\n\n- Stream request body as bytes or text chunks\n- Support chunked transfer encoding on input\n- Handle different content types (multipart, JSON arrays, binary)\n- Provide progress indication for large uploads\n- Support early termination of uploads on errors\n- Memory-efficient handling of large bodies\n\n## Acceptance Criteria\n\n- [ ] Large request bodies can be streamed\n- [ ] Memory usage remains constant for large bodies\n- [ ] Chunked encoding supported\n- [ ] Different content types handled\n- [ ] Early termination works on errors\n- [ ] Progress tracking available\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 6,
            "title": "Support streaming response bodies",
            "description": "Enable streaming large HTTP response bodies from pipeline outputs",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nFor large response bodies (e.g., large datasets, file downloads), we should support streaming the response content from pipeline outputs without buffering everything in memory.\n\n## Requirements\n\n- Stream pipeline outputs as response body chunks\n- Support chunked transfer encoding on output\n- Set appropriate Content-Type headers\n- Handle streaming errors gracefully\n- Support early client disconnection\n- Memory-efficient handling of large responses\n\n## Acceptance Criteria\n\n- [ ] Large response bodies can be streamed\n- [ ] Memory usage remains constant for large responses\n- [ ] Chunked encoding supported\n- [ ] Content-Type headers set correctly\n- [ ] Client disconnection handled gracefully\n- [ ] Streaming errors handled properly\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 7,
            "title": "Add middleware support",
            "description": "Enable Axum middleware for authentication, CORS, logging, and rate limiting",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "## Context\n\nProduction REST APIs require middleware for common concerns like authentication, CORS, request logging, rate limiting, etc. We should support Axum's middleware ecosystem.\n\n## Requirements\n\n- Support Axum middleware tower layers\n- Common middleware examples (CORS, auth, logging)\n- Middleware execution order\n- Request context passing through middleware\n- Error handling in middleware\n- Integration with StreamWeave error strategies\n\n## Acceptance Criteria\n\n- [ ] Axum middleware can be applied to routes\n- [ ] CORS middleware works correctly\n- [ ] Authentication middleware can extract tokens/credentials\n- [ ] Logging middleware captures request/response info\n- [ ] Rate limiting middleware works\n- [ ] Request context flows through middleware\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 8,
            "title": "Add comprehensive error handling for HTTP",
            "description": "Map StreamWeave errors to appropriate HTTP status codes and error responses",
            "status": "done",
            "dependencies": [
              4
            ],
            "details": "## Context\n\nStreamWeave has comprehensive error handling, but HTTP responses need specific status codes and error message formats. We need to map pipeline errors to HTTP errors.\n\n## Requirements\n\n- Map StreamWeave errors to HTTP status codes\n- Format error responses as JSON\n- Include error context in responses\n- Handle different error types (validation, not found, server errors)\n- Support custom error responses\n- Preserve error details in development mode\n\n## Acceptance Criteria\n\n- [ ] Validation errors return 400 Bad Request\n- [ ] Not found errors return 404 Not Found\n- [ ] Server errors return 500 Internal Server Error\n- [ ] Error responses include useful messages\n- [ ] Error context available in development\n- [ ] Custom error responses supported\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          },
          {
            "id": 9,
            "title": "Create REST microservice example",
            "description": "Build complete example REST API demonstrating all HTTP server capabilities",
            "status": "done",
            "dependencies": [
              4,
              5,
              6,
              7,
              8
            ],
            "details": "## Context\n\nA comprehensive example helps users understand how to build REST microservices with StreamWeave, demonstrating best practices and common patterns.\n\n## Requirements\n\n- Example REST API with multiple endpoints\n- Demonstrate GET, POST, PUT, DELETE operations\n- Show request/response body streaming\n- Include database integration example\n- Show error handling patterns\n- Include middleware usage (CORS, logging)\n- Demonstrate concurrent request handling\n- Include documentation and comments\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Multiple REST endpoints implemented\n- [ ] All HTTP methods demonstrated\n- [ ] Request/response streaming shown\n- [ ] Database integration included\n- [ ] Error handling demonstrated\n- [ ] Middleware usage shown\n- [ ] Well-commented and documented\n- [ ] Example demonstrating the functionality is created and runs successfully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete\n\n## Documentation Requirements\n\nDoxidize documentation should be generated for all implemented code in this task. This includes:\n- Comprehensive doc comments for all public APIs\n- Code examples in doc comments where appropriate\n- Module-level documentation for public modules\n- Documentation should be generated using `./bin/docs` and verified to build successfully\n- All public types, functions, and methods should have complete documentation",
            "testStrategy": "- [ ] Doxidize documentation is generated successfully using `./bin/docs` and all public APIs are documented"
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-25T23:37:23.909Z",
      "updated": "2025-11-27T12:05:07.979Z",
      "description": "Tasks for master context"
    }
  }
}