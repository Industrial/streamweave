{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Implement Stateful Processing",
        "description": "Add support for stateful stream processing where transformers can maintain and update state across items.",
        "status": "done",
        "priority": "high",
        "dependencies": [],
        "details": "## Context\n\nStateful processing is essential for many stream processing use cases like aggregations, sessionization, and pattern detection. Currently, StreamWeave transformers are stateless - they process each item independently without memory of previous items.\n\n## Implementation Approach\n\n1. Create a `StatefulTransformer` trait extending the base `Transformer` trait\n2. Add state storage abstraction (in-memory initially, with hooks for external stores)\n3. Implement state lifecycle management (initialization, updates, cleanup)\n4. Add state serialization for checkpointing\n\n## Technical Considerations\n\n- State must be thread-safe for concurrent access\n- Memory management for large state\n- State migration between transformer versions\n- Integration with error handling strategies",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] `StatefulTransformer` trait is defined with state type parameter\n- [ ] State can be initialized, read, updated, and cleared\n- [ ] State persists across multiple items in the stream\n- [ ] State is properly cleaned up when pipeline completes\n- [ ] Running sum/average transformer example works correctly\n- [ ] State snapshots can be taken and restored\n- [ ] Unit tests achieve >90% coverage\n- [ ] Documentation includes usage examples\n- [ ] Performance benchmark shows <10% overhead vs stateless",
        "subtasks": [
          {
            "id": 1,
            "title": "Design StatefulTransformer trait",
            "description": "Design and implement the core StatefulTransformer trait with state type parameter",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nThe StatefulTransformer trait needs to extend the existing Transformer trait while adding state management capabilities. It should be generic over the state type and provide methods for state access.\n\n## Requirements\n\n- Generic state type parameter with appropriate bounds (Send, Sync, Clone)\n- Methods: get_state(), update_state(), reset_state()\n- Integration with existing transform() method\n- State initialization hook\n\n## Acceptance Criteria\n\n- [ ] Trait compiles and is ergonomic to implement\n- [ ] State type is generic and flexible\n- [ ] Trait is compatible with existing pipeline builder\n- [ ] Documentation explains state lifecycle"
          },
          {
            "id": 2,
            "title": "Implement in-memory state storage",
            "description": "Create thread-safe in-memory state storage implementation",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nThe default state storage should be in-memory using appropriate Rust synchronization primitives.\n\n## Requirements\n\n- Use Arc<RwLock<S>> or similar for thread-safe access\n- Support atomic state updates\n- Implement StateStore trait for abstraction\n\n## Acceptance Criteria\n\n- [ ] Thread-safe concurrent read/write access\n- [ ] No deadlocks under concurrent access patterns\n- [ ] Memory is properly freed on cleanup\n- [ ] Performance is acceptable for high-throughput streams"
          },
          {
            "id": 3,
            "title": "Create example stateful transformers",
            "description": "Implement RunningSum, MovingAverage, and SessionWindow as example stateful transformers",
            "status": "done",
            "dependencies": [
              1,
              2
            ],
            "details": "## Context\n\nExample implementations help users understand the API and serve as integration tests.\n\n## Requirements\n\n- RunningSumTransformer: maintains cumulative sum\n- MovingAverageTransformer: configurable window size\n- Add to transformers module\n\n## Acceptance Criteria\n\n- [ ] All examples compile and work correctly\n- [ ] Examples are well-documented with doc comments\n- [ ] Examples appear in generated documentation\n- [ ] Tests verify correct behavior"
          },
          {
            "id": 4,
            "title": "Add state checkpointing",
            "description": "Implement state serialization and checkpointing for recovery",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nCheckpointing allows state to be saved and restored for fault tolerance.\n\n## Requirements\n\n- Serialize state to bytes using serde\n- Save checkpoints to configurable location\n- Restore state from checkpoint on startup\n\n## Acceptance Criteria\n\n- [ ] State can be serialized to JSON/bincode\n- [ ] Checkpoints can be saved to file\n- [ ] State can be restored from checkpoint\n- [ ] Checkpoint interval is configurable"
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Exactly-Once Processing",
        "description": "Implement exactly-once processing guarantees for reliable stream processing with deduplication and idempotency.",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "## Context\n\nExactly-once semantics ensure each record is processed exactly once, even in the presence of failures. This is critical for financial transactions, inventory updates, and other business-critical operations.\n\n## Implementation Approach\n\n1. Implement message IDs and deduplication\n2. Add transaction support for atomic commits\n3. Implement offset tracking for resumable processing\n4. Add checkpointing integration\n\n## Technical Considerations\n\n- Trade-off between latency and exactly-once guarantees\n- Storage requirements for deduplication state\n- Integration with external systems (databases, message queues)",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Each message has a unique identifier\n- [ ] Duplicate messages are detected and skipped\n- [ ] Pipeline can resume from last checkpoint after crash\n- [ ] Offset tracking persists across restarts\n- [ ] Transaction rollback on failure works correctly\n- [ ] No data loss or duplication in failure scenarios\n- [ ] Performance impact is documented\n- [ ] Integration tests cover failure scenarios",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement message ID tracking",
            "description": "Add unique message IDs to stream items for deduplication",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMessage IDs are the foundation of exactly-once processing.\n\n## Requirements\n\n- Generate unique IDs (UUID or sequence)\n- Attach ID to each stream item\n- Make ID accessible in transformers\n\n## Acceptance Criteria\n\n- [ ] All items have unique IDs\n- [ ] IDs are preserved through transformations\n- [ ] IDs can be extracted for deduplication"
          },
          {
            "id": 2,
            "title": "Create deduplication transformer",
            "description": "Implement a transformer that filters duplicate messages based on ID",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nDeduplication removes repeated messages that may occur due to retries.\n\n## Requirements\n\n- Track seen message IDs\n- Configurable dedup window (time or count based)\n- Memory-efficient ID storage (bloom filter option)\n\n## Acceptance Criteria\n\n- [ ] Duplicates are correctly identified and filtered\n- [ ] Window expiration works correctly\n- [ ] Memory usage is bounded\n- [ ] Performance is acceptable for high-throughput"
          },
          {
            "id": 3,
            "title": "Implement offset tracking",
            "description": "Track processing offsets for resumable pipelines",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nOffset tracking allows pipelines to resume from where they left off.\n\n## Requirements\n\n- Track last processed offset per source\n- Persist offsets to storage\n- Support offset commit strategies (auto, manual)\n\n## Acceptance Criteria\n\n- [ ] Offsets are tracked accurately\n- [ ] Offsets persist across restarts\n- [ ] Manual and auto commit modes work\n- [ ] Offset reset (earliest/latest) is supported"
          },
          {
            "id": 4,
            "title": "Add transaction support",
            "description": "Implement transactional processing for atomic commits",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nTransactions ensure all-or-nothing processing for batches.\n\n## Requirements\n\n- Transaction begin/commit/rollback API\n- Integration with offset commits\n- Configurable transaction timeout\n\n## Acceptance Criteria\n\n- [ ] Transactions can be started and committed\n- [ ] Rollback undoes partial processing\n- [ ] Timeouts prevent stuck transactions\n- [ ] Nested transactions handled correctly"
          }
        ]
      },
      {
        "id": 3,
        "title": "Implement Windowing Operations",
        "description": "Implement comprehensive windowing operations for time-based and count-based stream processing.",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "## Context\n\nWindowing is fundamental to stream processing, allowing bounded computations over unbounded streams. Windows group elements for aggregation based on time or count.\n\n## Implementation Approach\n\n1. Define Window trait and common window types\n2. Implement tumbling, sliding, and session windows\n3. Add watermark support for event-time processing\n4. Handle late data with configurable policies\n\n## Technical Considerations\n\n- Memory management for window state\n- Timer management for window triggers\n- Trade-offs between latency and completeness",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Tumbling windows work with time and count triggers\n- [ ] Sliding windows handle overlapping correctly\n- [ ] Session windows detect gaps and close appropriately\n- [ ] Watermarks track event-time progress\n- [ ] Late data is handled per configured policy\n- [ ] Window state is garbage collected after closing\n- [ ] Performance scales with window count\n- [ ] Examples demonstrate each window type",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Window trait and types",
            "description": "Create the core Window trait and enum for window types",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nA well-designed Window abstraction enables flexible windowing strategies.\n\n## Requirements\n\n- Window trait with assign(), trigger(), merge() methods\n- WindowAssigner for assigning elements to windows\n- WindowTrigger for determining when to emit\n\n## Acceptance Criteria\n\n- [ ] Trait is flexible enough for all window types\n- [ ] Type system prevents invalid window configurations\n- [ ] API is ergonomic and well-documented"
          },
          {
            "id": 2,
            "title": "Implement TumblingWindow",
            "description": "Create non-overlapping fixed-size windows",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nTumbling windows are the simplest window type - fixed size, non-overlapping.\n\n## Requirements\n\n- Time-based tumbling windows\n- Count-based tumbling windows\n- Configurable window size\n\n## Acceptance Criteria\n\n- [ ] Windows don't overlap\n- [ ] All elements assigned to exactly one window\n- [ ] Window closes after trigger\n- [ ] Late elements handled per policy"
          },
          {
            "id": 3,
            "title": "Implement SlidingWindow",
            "description": "Create overlapping windows with configurable slide",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nSliding windows allow elements to belong to multiple windows.\n\n## Requirements\n\n- Configurable window size and slide interval\n- Elements assigned to all applicable windows\n- Efficient memory management\n\n## Acceptance Criteria\n\n- [ ] Elements appear in correct number of windows\n- [ ] Slide interval works correctly\n- [ ] Memory doesn't grow unbounded"
          },
          {
            "id": 4,
            "title": "Implement SessionWindow",
            "description": "Create gap-based session windows",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nSession windows group elements with gaps smaller than a threshold.\n\n## Requirements\n\n- Configurable session gap timeout\n- Dynamic window boundaries\n- Session merging when gaps fill in\n\n## Acceptance Criteria\n\n- [ ] Sessions close after gap timeout\n- [ ] Adjacent sessions merge correctly\n- [ ] Late arrivals extend sessions properly"
          },
          {
            "id": 5,
            "title": "Implement watermarks and late data handling",
            "description": "Add event-time tracking and late data policies",
            "status": "done",
            "dependencies": [
              2,
              3,
              4
            ],
            "details": "## Context\n\nWatermarks track progress in event time, enabling handling of out-of-order data.\n\n## Requirements\n\n- Watermark generation strategies\n- Late data policies (drop, emit, sideOutput)\n- Allowed lateness configuration\n\n## Acceptance Criteria\n\n- [ ] Watermarks advance correctly\n- [ ] Late data identified accurately\n- [ ] All policies implemented and tested"
          }
        ]
      },
      {
        "id": 4,
        "title": "Add Specialized Transformers",
        "description": "Implement additional specialized transformers for common stream processing operations.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nA rich set of transformers makes the framework more useful out of the box. These are common operations that users shouldn't need to implement themselves.\n\n## Implementation Approach\n\n1. Implement each transformer following existing patterns\n2. Ensure consistent API and error handling\n3. Add comprehensive tests and documentation\n\n## Technical Considerations\n\n- Maintain consistency with existing transformer API\n- Consider memory usage for stateful transformers\n- Ensure WASM compatibility",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] All planned transformers implemented\n- [ ] Each transformer has >90% test coverage\n- [ ] All transformers have doc comments with examples\n- [ ] Performance benchmarks for each transformer\n- [ ] All transformers work in WASM builds",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement FlatMap transformer",
            "description": "Create transformer that maps one input to zero or more outputs",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nFlatMap is essential for one-to-many transformations like splitting strings.\n\n## Requirements\n\n- Accept function that returns iterator\n- Flatten results into output stream\n- Handle empty results gracefully\n\n## Acceptance Criteria\n\n- [ ] One-to-many mapping works correctly\n- [ ] Empty results don't break stream\n- [ ] Type inference works well\n- [ ] Performance is comparable to manual implementation"
          },
          {
            "id": 2,
            "title": "Implement Reduce transformer",
            "description": "Create transformer for aggregating stream elements",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nReduce combines elements using an associative function.\n\n## Requirements\n\n- Accept binary reduction function\n- Support initial accumulator value\n- Emit intermediate or final results\n\n## Acceptance Criteria\n\n- [ ] Reduction produces correct result\n- [ ] Works with various types\n- [ ] Can emit running results if configured"
          },
          {
            "id": 3,
            "title": "Implement GroupBy transformer",
            "description": "Create transformer for grouping elements by key",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nGroupBy partitions streams by key for per-key processing.\n\n## Requirements\n\n- Accept key extraction function\n- Route elements to per-key sub-streams\n- Support downstream per-key aggregation\n\n## Acceptance Criteria\n\n- [ ] Elements grouped by correct key\n- [ ] Per-key processing is isolated\n- [ ] Memory bounded for many keys"
          },
          {
            "id": 4,
            "title": "Implement Join transformer",
            "description": "Create transformer for joining two streams",
            "status": "done",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nJoins combine elements from two streams based on keys.\n\n## Requirements\n\n- Inner join implementation\n- Configurable join window\n- Key-based matching\n\n## Acceptance Criteria\n\n- [ ] Matching elements are correctly joined\n- [ ] Window limits memory usage\n- [ ] Unmatched elements handled per config"
          },
          {
            "id": 5,
            "title": "Implement Distinct transformer",
            "description": "Create transformer that emits only unique elements",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDistinct removes duplicates from the stream.\n\n## Requirements\n\n- Track seen elements\n- Configurable uniqueness key\n- Memory-bounded tracking (optional bloom filter)\n\n## Acceptance Criteria\n\n- [ ] Duplicates correctly filtered\n- [ ] Custom key extraction works\n- [ ] Memory usage is bounded"
          },
          {
            "id": 6,
            "title": "Implement Sample transformer",
            "description": "Create transformer for statistical sampling",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nSampling reduces stream volume for testing or analysis.\n\n## Requirements\n\n- Random sampling with configurable rate\n- Reservoir sampling for fixed-size samples\n- Reproducible with seed\n\n## Acceptance Criteria\n\n- [ ] Sample rate is accurate over time\n- [ ] Distribution is uniform\n- [ ] Seeded sampling is reproducible"
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Fan-In/Fan-Out Support",
        "description": "Add support for splitting streams to multiple consumers and merging multiple streams.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nFan-out allows one stream to feed multiple downstream processors. Fan-in merges multiple streams into one. These are essential for parallel processing and stream routing.\n\n## Implementation Approach\n\n1. Design stream splitting mechanism\n2. Implement various distribution strategies\n3. Add stream merging with ordering options\n4. Create routing based on content\n\n## Technical Considerations\n\n- Backpressure propagation across branches\n- Ordering guarantees in fan-in\n- Memory management for buffering",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Stream can be split to multiple consumers\n- [ ] All distribution strategies work correctly\n- [ ] Merged streams maintain configured ordering\n- [ ] Content-based routing works correctly\n- [ ] Backpressure propagates correctly\n- [ ] No data loss during fan-out/fan-in\n- [ ] Performance scales linearly with branch count",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement stream broadcast",
            "description": "Send each element to all downstream consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nBroadcast sends every element to all consumers.\n\n## Requirements\n\n- Clone elements for each consumer\n- Handle slow consumers\n- Configurable buffer size\n\n## Acceptance Criteria\n\n- [ ] All consumers receive all elements\n- [ ] Slow consumers don't block fast ones\n- [ ] Memory usage is bounded"
          },
          {
            "id": 2,
            "title": "Implement round-robin distribution",
            "description": "Distribute elements evenly across consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nRound-robin load balances across consumers.\n\n## Requirements\n\n- Cycle through consumers in order\n- Handle consumer addition/removal\n- Even distribution over time\n\n## Acceptance Criteria\n\n- [ ] Distribution is even\n- [ ] Dynamic consumer changes handled\n- [ ] No element loss during rebalance"
          },
          {
            "id": 3,
            "title": "Implement content-based routing",
            "description": "Route elements based on content to specific consumers",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nContent-based routing directs elements based on their properties.\n\n## Requirements\n\n- Accept routing function\n- Support default route\n- Handle unroutable elements\n\n## Acceptance Criteria\n\n- [ ] Elements route to correct consumer\n- [ ] Default route catches unmatched\n- [ ] Routing function errors handled"
          },
          {
            "id": 4,
            "title": "Implement stream merge",
            "description": "Combine multiple input streams into one output",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMerge combines multiple streams into a single output.\n\n## Requirements\n\n- Merge with configurable ordering (FIFO, round-robin, priority)\n- Handle stream completion\n- Propagate errors from any source\n\n## Acceptance Criteria\n\n- [ ] All elements from all streams appear in output\n- [ ] Ordering matches configuration\n- [ ] Completed streams don't block others"
          }
        ]
      },
      {
        "id": 6,
        "title": "Add Data Format Support",
        "description": "Add support for additional data formats including JSON streaming, CSV, Parquet, and more.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nSupporting common data formats makes the framework more useful for real-world data processing tasks.\n\n## Implementation Approach\n\n1. Create format-specific producers and consumers\n2. Add serialization/deserialization transformers\n3. Integrate with serde ecosystem\n4. Support streaming parsing for large files\n\n## Technical Considerations\n\n- Memory efficiency for large files\n- Streaming vs buffered parsing\n- Error handling for malformed data",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] JSON streaming works with large files\n- [ ] CSV parsing handles various dialects\n- [ ] Parquet read/write implemented\n- [ ] All formats integrate with serde\n- [ ] Streaming parsing doesn't load entire file\n- [ ] Malformed data handled gracefully\n- [ ] Performance comparable to dedicated libraries",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement JSON streaming",
            "description": "Add JSON Lines producer and consumer with serde integration",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nJSON Lines (JSONL) is a common format for streaming JSON data.\n\n## Requirements\n\n- Parse newline-delimited JSON\n- Integrate with serde for type-safe parsing\n- Stream large files without loading all into memory\n\n## Acceptance Criteria\n\n- [ ] JSONL files parsed correctly\n- [ ] Serde deserialization works\n- [ ] Memory usage constant regardless of file size\n- [ ] Parse errors handled per element"
          },
          {
            "id": 2,
            "title": "Implement CSV support",
            "description": "Add CSV producer and consumer with header handling",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nCSV is ubiquitous for tabular data.\n\n## Requirements\n\n- Parse CSV with configurable delimiter\n- Handle headers and type inference\n- Support quoted fields and escaping\n\n## Acceptance Criteria\n\n- [ ] Various CSV dialects parsed correctly\n- [ ] Headers extracted and used for field names\n- [ ] Streaming parsing for large files\n- [ ] Write support with proper escaping"
          },
          {
            "id": 3,
            "title": "Implement Parquet support",
            "description": "Add Parquet file producer and consumer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nParquet is efficient for analytics workloads.\n\n## Requirements\n\n- Read Parquet files with column selection\n- Write Parquet with compression\n- Support row group streaming\n\n## Acceptance Criteria\n\n- [ ] Parquet files read correctly\n- [ ] Column projection pushdown works\n- [ ] Compression options configurable\n- [ ] Large files don't exhaust memory"
          },
          {
            "id": 4,
            "title": "Implement MessagePack support",
            "description": "Add MessagePack serialization transformer",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nMessagePack is a compact binary format.\n\n## Requirements\n\n- Serialize to MessagePack\n- Deserialize from MessagePack\n- Integrate with serde\n\n## Acceptance Criteria\n\n- [ ] Round-trip serialization works\n- [ ] Performance better than JSON\n- [ ] Size smaller than JSON equivalent"
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement Distributed Processing",
        "description": "Support distributed stream processing across multiple nodes for horizontal scalability.",
        "status": "done",
        "priority": "low",
        "dependencies": [
          1,
          2,
          5
        ],
        "details": "## Context\n\nDistributed processing enables horizontal scaling for high-volume streams that exceed single-node capacity.\n\n## Implementation Approach\n\n1. Design coordinator/worker architecture\n2. Implement data partitioning strategies\n3. Add network communication layer\n4. Implement fault tolerance and recovery\n\n## Technical Considerations\n\n- Network partition handling\n- Exactly-once semantics across nodes\n- Shuffle and redistribution costs\n- State management in distributed setting",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Pipelines run across multiple nodes\n- [ ] Data partitioned correctly by key\n- [ ] Worker failures handled with recovery\n- [ ] Coordinator failover works\n- [ ] Network partitions handled gracefully\n- [ ] Performance scales linearly with nodes\n- [ ] Exactly-once preserved across cluster\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design distributed architecture",
            "description": "Create architecture document for distributed processing",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nA solid architecture is essential before implementation.\n\n## Requirements\n\n- Define coordinator and worker roles\n- Specify communication protocol\n- Plan state distribution strategy\n- Document failure modes and recovery\n\n## Acceptance Criteria\n\n- [ ] Architecture document reviewed and approved\n- [ ] All failure modes identified\n- [ ] Protocol defined with message formats\n- [ ] Scalability limits documented"
          },
          {
            "id": 2,
            "title": "Implement data partitioning",
            "description": "Create partitioning strategies for distributing data",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nPartitioning determines how data is distributed across workers.\n\n## Requirements\n\n- Hash-based partitioning by key\n- Range partitioning\n- Custom partitioner support\n\n## Acceptance Criteria\n\n- [ ] Consistent partitioning (same key -> same worker)\n- [ ] Even distribution across workers\n- [ ] Rebalancing on worker change\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Implement network communication",
            "description": "Add gRPC or custom protocol for inter-node communication",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nEfficient network communication is critical for distributed processing.\n\n## Requirements\n\n- gRPC for RPC communication\n- Streaming data transfer\n- Connection pooling and retry\n\n## Acceptance Criteria\n\n- [ ] Nodes can discover and connect\n- [ ] Data streams efficiently between nodes\n- [ ] Connection failures handled with reconnect\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Implement fault tolerance",
            "description": "Add worker failure detection and recovery",
            "status": "done",
            "dependencies": [
              2,
              3
            ],
            "details": "## Context\n\nFault tolerance keeps the system running despite failures.\n\n## Requirements\n\n- Heartbeat-based failure detection\n- Work redistribution on failure\n- State recovery from checkpoints\n\n## Acceptance Criteria\n\n- [ ] Failed workers detected within timeout\n- [ ] Work redistributed to healthy workers\n- [ ] No data loss on worker failure\n- [ ] System continues processing after recovery\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 8,
        "title": "WASM Optimizations",
        "description": "Optimize StreamWeave for WebAssembly deployment with browser and server-side WASM support.",
        "status": "done",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nWASM support enables running pipelines in browsers and edge environments. The current codebase supports WASM but needs optimization.\n\n## Implementation Approach\n\n1. Audit current WASM compatibility\n2. Add feature flags for WASM-specific code\n3. Optimize bundle size\n4. Create WASM-specific documentation\n\n## Technical Considerations\n\n- No std library for some targets\n- Memory management differences\n- Async runtime compatibility\n- Bundle size concerns",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] All core features work in WASM\n- [ ] Bundle size under 500KB gzipped\n- [ ] Memory usage acceptable in browser\n- [ ] Examples work in browser and Node.js\n- [ ] Documentation covers WASM deployment\n- [ ] CI tests WASM targets\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Audit WASM compatibility",
            "description": "Test all features in WASM and document incompatibilities",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nNeed to understand current WASM support level.\n\n## Requirements\n\n- Test each module in wasm32 target\n- Document what works and what doesn't\n- Identify dependencies blocking WASM\n\n## Acceptance Criteria\n\n- [ ] Compatibility matrix documented\n- [ ] Blocking issues identified\n- [ ] Plan for fixes created"
          },
          {
            "id": 2,
            "title": "Add WASM feature flags",
            "description": "Create feature flags for WASM-specific code paths",
            "status": "done",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nFeature flags allow conditional compilation for WASM.\n\n## Requirements\n\n- 'wasm' feature flag in Cargo.toml\n- Conditional compilation for incompatible code\n- WASM-specific implementations where needed\n\n## Acceptance Criteria\n\n- [ ] Feature flag controls WASM-specific code\n- [ ] Native build unaffected by WASM code\n- [ ] Both targets compile cleanly"
          },
          {
            "id": 3,
            "title": "Optimize bundle size",
            "description": "Reduce WASM bundle size through optimization",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nSmall bundle size is important for web deployment.\n\n## Requirements\n\n- Enable LTO and size optimization\n- Remove unused code with wasm-opt\n- Optional features for code splitting\n\n## Acceptance Criteria\n\n- [ ] Bundle under 500KB gzipped\n- [ ] Core features available in minimal build\n- [ ] Size regression tests in CI\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Create WASM documentation and examples",
            "description": "Write WASM-specific docs and browser examples",
            "status": "done",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nUsers need guidance for WASM deployment.\n\n## Requirements\n\n- Getting started guide for WASM\n- Browser example with webpack/vite\n- Node.js example\n- Performance tips\n\n## Acceptance Criteria\n\n- [ ] WASM docs in README or separate file\n- [ ] Working browser example\n- [ ] Working Node.js example\n- [ ] Common issues documented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 9,
        "title": "Add Specialized Producers and Consumers",
        "description": "Implement producers and consumers for common external systems like Kafka, Redis, and databases.",
        "status": "pending",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nIntegration with external systems makes StreamWeave useful for real-world applications. These are the most commonly requested integrations.\n\n## Implementation Approach\n\n1. Define standard interfaces for external systems\n2. Implement each integration as optional feature\n3. Ensure consistent configuration patterns\n4. Add integration tests with testcontainers\n\n## Technical Considerations\n\n- Connection pooling and management\n- Retry and reconnection logic\n- Backpressure handling\n- Configuration consistency",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Kafka producer/consumer work with real cluster\n- [ ] Redis Streams integration works\n- [ ] Database query producer handles large results\n- [ ] All integrations have retry logic\n- [ ] Configuration is consistent across integrations\n- [ ] Integration tests pass in CI\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Kafka producer/consumer",
            "description": "Add Kafka integration for producing and consuming messages",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nKafka is the most common streaming platform.\n\n## Requirements\n\n- Consume from Kafka topics\n- Produce to Kafka topics\n- Support consumer groups\n- Handle partition assignment\n\n## Acceptance Criteria\n\n- [ ] Can consume from multiple partitions\n- [ ] Offset management works\n- [ ] Producer batching configurable\n- [ ] Connection failures handled\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Implement Redis Streams producer/consumer",
            "description": "Add Redis Streams integration",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nRedis Streams provides lightweight message streaming.\n\n## Requirements\n\n- XREAD for consuming\n- XADD for producing\n- Consumer group support\n- Message acknowledgment\n\n## Acceptance Criteria\n\n- [ ] Can read from streams\n- [ ] Can write to streams\n- [ ] Consumer groups work\n- [ ] Pending messages tracked\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Implement database query producer",
            "description": "Add SQL database producer for query results",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nDatabase queries are a common data source.\n\n## Requirements\n\n- Support PostgreSQL and MySQL\n- Cursor-based iteration for large results\n- Parameterized queries\n- Connection pooling\n\n## Acceptance Criteria\n\n- [ ] Large query results streamed efficiently\n- [ ] Memory usage bounded\n- [ ] Connection pool managed correctly\n- [ ] Multiple databases supported\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Implement HTTP polling producer",
            "description": "Add producer that polls HTTP endpoints",
            "status": "done",
            "dependencies": [],
            "details": "## Context\n\nPolling APIs is common for integrations without webhooks.\n\n## Requirements\n\n- Configurable poll interval\n- Support for pagination\n- Delta detection (only new items)\n- Rate limiting\n\n## Acceptance Criteria\n\n- [ ] Polls at configured interval\n- [ ] Handles pagination correctly\n- [ ] Emits only new items\n- [ ] Rate limits respected\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 10,
        "title": "Create Comprehensive Examples",
        "description": "Create examples demonstrating all major functionality to help users understand and adopt StreamWeave features.",
        "status": "pending",
        "priority": "medium",
        "dependencies": [
          9
        ],
        "details": "## Context\n\nExamples are essential for helping users understand how to use StreamWeave effectively. While we have basic and advanced pipeline examples, many key features lack dedicated examples showing real-world usage patterns.\n\n## Implementation Approach\n\n1. Create example for each major integration (Kafka, Redis, Database, HTTP)\n2. Add examples for advanced transformers and patterns\n3. Create examples demonstrating error handling and resilience patterns\n4. Update README.md to reflect current capabilities\n\n## Technical Considerations\n\n- Examples should be runnable and well-documented\n- Use realistic scenarios that demonstrate value\n- Include setup instructions for external dependencies\n- Show both simple and complex use cases\n- Each example should have a README with explanation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Each example compiles and runs successfully\n- [ ] Examples demonstrate real-world usage patterns\n- [ ] All examples have README with setup instructions\n- [ ] README.md accurately reflects current feature set\n- [ ] Examples are listed in main README\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Kafka integration example",
            "description": "Add example demonstrating Kafka producer and consumer",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nKafka is one of the most common message streaming platforms. Users need a clear example showing how to consume from and produce to Kafka topics.\n\n## Requirements\n\n- Demonstrate consuming from Kafka topics with consumer groups\n- Show producing messages to Kafka with batching\n- Include setup instructions for local Kafka instance\n- Show offset management and error handling\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] README explains Kafka setup\n- [ ] Demonstrates consumer groups and partition handling\n- [ ] Shows producer batching configuration\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Create Redis Streams integration example",
            "description": "Add example demonstrating Redis Streams producer and consumer",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nRedis Streams provides lightweight message streaming. An example helps users understand the integration.\n\n## Requirements\n\n- Demonstrate XREAD and XADD operations\n- Show consumer group usage\n- Include message acknowledgment patterns\n- Show setup for local Redis instance\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] README explains Redis setup\n- [ ] Demonstrates consumer groups\n- [ ] Shows message acknowledgment\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Create database query example",
            "description": "Add example demonstrating database query producer with SQLite",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nDatabase queries are a common data source. Users need examples showing how to stream query results efficiently.\n\n## Requirements\n\n- Use in-memory SQLite for easy testing\n- Demonstrate parameterized queries\n- Show cursor-based streaming for large results\n- Include connection pooling configuration\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Uses in-memory SQLite (no external setup needed)\n- [ ] Demonstrates parameterized queries\n- [ ] Shows streaming large result sets\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Create HTTP polling example",
            "description": "Add example demonstrating HTTP polling producer with pagination",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nHTTP polling is common for APIs without webhooks. Users need examples showing polling, pagination, and delta detection.\n\n## Requirements\n\n- Demonstrate polling an HTTP endpoint\n- Show pagination handling (query params, Link headers, or JSON fields)\n- Include delta detection to emit only new items\n- Show rate limiting configuration\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates pagination strategies\n- [ ] Shows delta detection in action\n- [ ] Includes rate limiting\n- [ ] Uses mock HTTP server or public API\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 5,
            "title": "Create file formats example",
            "description": "Add example demonstrating CSV, JSONL, and Parquet producers/consumers",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nFile format support is essential for data processing. Users need examples showing how to read and write various formats.\n\n## Requirements\n\n- Demonstrate CSV read/write with headers\n- Show JSONL streaming for large files\n- Include Parquet read/write with column projection\n- Show streaming parsing to avoid loading entire files\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates CSV with header handling\n- [ ] Shows JSONL streaming\n- [ ] Includes Parquet column projection\n- [ ] Uses test data files in example directory\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 6,
            "title": "Create stateful processing example",
            "description": "Add example demonstrating stateful transformers (RunningSum, MovingAverage)",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nStateful processing enables aggregations and analytics. Users need examples showing how state is maintained across items.\n\n## Requirements\n\n- Demonstrate RunningSumTransformer for cumulative calculations\n- Show MovingAverageTransformer with configurable window size\n- Include state checkpointing example\n- Explain state lifecycle and cleanup\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates RunningSum\n- [ ] Shows MovingAverage with different window sizes\n- [ ] Includes checkpoint example if applicable\n- [ ] Explains state management clearly\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 7,
            "title": "Create error handling example",
            "description": "Add example demonstrating error handling strategies (Stop, Skip, Retry, Custom)",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nError handling is critical for production pipelines. Users need examples showing different strategies and when to use them.\n\n## Requirements\n\n- Demonstrate Stop, Skip, Retry, and Custom error strategies\n- Show pipeline-level and component-level error handling\n- Include examples of error recovery patterns\n- Show how to handle different error types\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates all error handling strategies\n- [ ] Shows pipeline and component-level configuration\n- [ ] Includes realistic error scenarios\n- [ ] Explains when to use each strategy\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 8,
            "title": "Create advanced transformers example",
            "description": "Add example demonstrating CircuitBreaker, Retry, Batch, and RateLimit transformers",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nAdvanced transformers provide resilience and performance optimization. Examples help users understand their use cases.\n\n## Requirements\n\n- Demonstrate CircuitBreaker for external service calls\n- Show Retry transformer with exponential backoff\n- Include Batch transformer for grouping items\n- Show RateLimit transformer for throughput control\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates CircuitBreaker with failure scenarios\n- [ ] Shows Retry with backoff strategies\n- [ ] Includes Batch with time and size triggers\n- [ ] Shows RateLimit configuration\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 9,
            "title": "Create windowing operations example",
            "description": "Add example demonstrating time-based and count-based windowing",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nWindowing is fundamental to stream processing. Users need examples showing different window types and use cases.\n\n## Requirements\n\n- Demonstrate tumbling windows (time and count-based)\n- Show sliding windows with overlap\n- Include session windows for gap-based grouping\n- Show watermark handling for event-time processing\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates tumbling windows\n- [ ] Shows sliding windows\n- [ ] Includes session windows\n- [ ] Explains window triggers and emissions\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 10,
            "title": "Create exactly-once processing example",
            "description": "Add example demonstrating message deduplication and exactly-once guarantees",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nExactly-once processing ensures reliable stream processing. Users need examples showing deduplication and idempotency patterns.\n\n## Requirements\n\n- Demonstrate MessageDedupeTransformer\n- Show offset tracking and resume from checkpoint\n- Include transaction example if applicable\n- Show how to handle duplicates in retry scenarios\n\n## Acceptance Criteria\n\n- [ ] Example compiles and runs\n- [ ] Demonstrates message deduplication\n- [ ] Shows offset tracking and recovery\n- [ ] Includes checkpoint example\n- [ ] Explains exactly-once semantics\n- [ ] Example is well-commented\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 11,
            "title": "Update README.md with current capabilities",
            "description": "Update README.md to accurately reflect all implemented features and provide links to examples",
            "status": "pending",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6,
              7,
              8,
              9,
              10
            ],
            "details": "## Context\n\nThe README.md currently lists many features as 'Planned' that are actually implemented. It needs to be updated to reflect the current state and guide users to appropriate examples.\n\n## Requirements\n\n- Move implemented features from 'Planned' to 'Implemented'\n- Add links to all new examples\n- Update feature list to include Kafka, Redis, Database, HTTP polling integrations\n- Add stateful processing, exactly-once, windowing to implemented list\n- Include example links in appropriate sections\n- Update code examples if needed\n\n## Acceptance Criteria\n\n- [ ] All implemented features accurately listed\n- [ ] Planned features list only truly unimplemented items\n- [ ] Examples are linked from relevant sections\n- [ ] Code examples are current and work\n- [ ] README provides clear navigation to examples\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Monitoring and Metrics",
        "description": "Add comprehensive observability with metrics collection, tracing, and monitoring integration.",
        "status": "pending",
        "priority": "medium",
        "dependencies": [],
        "details": "## Context\n\nObservability is essential for production deployments. Users need to understand pipeline performance and health.\n\n## Implementation Approach\n\n1. Define standard metrics (throughput, latency, errors)\n2. Integrate with OpenTelemetry\n3. Add Prometheus export\n4. Support distributed tracing\n\n## Technical Considerations\n\n- Minimal overhead for metrics collection\n- Cardinality management\n- Trace context propagation",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Throughput, latency, error metrics collected\n- [ ] Metrics exported to Prometheus\n- [ ] OpenTelemetry traces generated\n- [ ] Trace context propagates through pipeline\n- [ ] Metrics overhead under 5%\n- [ ] Health check endpoint available\n- [ ] Dashboard templates provided\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Define standard metrics",
            "description": "Create standard metrics for pipeline monitoring",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nStandard metrics enable consistent monitoring.\n\n## Requirements\n\n- Throughput (items/second)\n- Latency (p50, p95, p99)\n- Error rate and types\n- Backpressure indicators\n\n## Acceptance Criteria\n\n- [ ] All standard metrics defined\n- [ ] Metrics have appropriate labels\n- [ ] Collection is efficient\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Implement OpenTelemetry integration",
            "description": "Add OpenTelemetry for traces and metrics",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nOpenTelemetry is the standard for observability.\n\n## Requirements\n\n- Trace spans for pipeline stages\n- Metrics via OTLP\n- Context propagation\n\n## Acceptance Criteria\n\n- [ ] Traces show pipeline execution\n- [ ] Metrics export via OTLP\n- [ ] Context propagates correctly\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Add Prometheus metrics export",
            "description": "Export metrics in Prometheus format",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nPrometheus is widely used for metrics.\n\n## Requirements\n\n- /metrics endpoint\n- Standard Prometheus format\n- Configurable labels\n\n## Acceptance Criteria\n\n- [ ] Metrics endpoint works\n- [ ] Prometheus can scrape metrics\n- [ ] Grafana dashboards work\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Implement health checks",
            "description": "Add health check endpoint and liveness probes",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nHealth checks enable orchestrator integration.\n\n## Requirements\n\n- /health endpoint\n- Liveness and readiness probes\n- Component health aggregation\n\n## Acceptance Criteria\n\n- [ ] Health endpoint returns status\n- [ ] Unhealthy components detected\n- [ ] Kubernetes probes work\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement SQL-Like Querying",
        "description": "Add SQL-like query interface for intuitive stream processing with SELECT, WHERE, GROUP BY syntax.",
        "status": "pending",
        "priority": "low",
        "dependencies": [
          3,
          4
        ],
        "details": "## Context\n\nSQL is familiar to many developers. A SQL-like interface lowers the learning curve for stream processing.\n\n## Implementation Approach\n\n1. Design SQL dialect for streams\n2. Implement SQL parser\n3. Translate SQL to pipeline operations\n4. Add query optimization\n\n## Technical Considerations\n\n- SQL semantics in streaming context\n- Query optimization opportunities\n- Type safety with SQL\n- Error messages for invalid queries",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] SELECT, WHERE, GROUP BY work\n- [ ] Aggregations produce correct results\n- [ ] JOIN works for windowed streams\n- [ ] Queries compile to efficient pipelines\n- [ ] Type errors caught at parse time\n- [ ] Query optimizer improves performance\n- [ ] Examples show common query patterns\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design SQL dialect",
            "description": "Define the SQL syntax for stream queries",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nNeed to define what SQL constructs are supported.\n\n## Requirements\n\n- SELECT for projection\n- WHERE for filtering\n- GROUP BY for aggregation\n- WINDOW for windowing\n- JOIN for stream joining\n\n## Acceptance Criteria\n\n- [ ] Syntax documented\n- [ ] Semantics clearly defined\n- [ ] Examples for each construct\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Implement SQL parser",
            "description": "Create parser for the SQL dialect",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nParser converts SQL text to AST.\n\n## Requirements\n\n- Use sqlparser-rs or custom parser\n- Parse all supported constructs\n- Helpful error messages\n\n## Acceptance Criteria\n\n- [ ] All syntax parsed correctly\n- [ ] Errors include position\n- [ ] Parser is well-tested\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Implement query to pipeline translation",
            "description": "Convert SQL AST to pipeline operations",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nTranslation converts SQL to efficient pipelines.\n\n## Requirements\n\n- Map SQL operations to transformers\n- Handle complex queries with multiple operations\n- Preserve semantics correctly\n\n## Acceptance Criteria\n\n- [ ] Simple queries translate correctly\n- [ ] Complex queries work\n- [ ] Result matches SQL semantics\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Add query optimization",
            "description": "Optimize query plans for better performance",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nOptimization improves query performance.\n\n## Requirements\n\n- Predicate pushdown\n- Projection pruning\n- Join ordering\n\n## Acceptance Criteria\n\n- [ ] Optimizer improves query performance\n- [ ] No semantic changes from optimization\n- [ ] Optimization rules are tested\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Machine Learning Integration",
        "description": "Add ML pipeline support with model inference, feature extraction, and integration with ML frameworks.",
        "status": "pending",
        "priority": "low",
        "dependencies": [
          1,
          4
        ],
        "details": "## Context\n\nML inference on streams enables real-time predictions. This is increasingly common in modern applications.\n\n## Implementation Approach\n\n1. Design inference transformer interface\n2. Integrate with ONNX Runtime\n3. Add batching for efficient inference\n4. Support model hot-swapping\n\n## Technical Considerations\n\n- Inference latency requirements\n- Batch size optimization\n- Model loading and memory\n- GPU support considerations",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] ONNX models load and run inference\n- [ ] Batching improves throughput\n- [ ] Model can be swapped without restart\n- [ ] Feature extraction utilities work\n- [ ] Latency acceptable for streaming use\n- [ ] Examples show end-to-end ML pipeline\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design inference transformer interface",
            "description": "Create trait for model inference transformers",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nA common interface enables different ML backends.\n\n## Requirements\n\n- Load model from path/bytes\n- Run inference on input\n- Support batched inference\n- Handle model errors\n\n## Acceptance Criteria\n\n- [ ] Trait is flexible for different frameworks\n- [ ] Batching is supported\n- [ ] Errors handled gracefully\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Implement ONNX Runtime integration",
            "description": "Add ONNX Runtime backend for inference",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nONNX is a portable model format.\n\n## Requirements\n\n- Load ONNX models\n- Run inference with ONNX Runtime\n- Support CPU and GPU execution\n\n## Acceptance Criteria\n\n- [ ] ONNX models load correctly\n- [ ] Inference produces correct results\n- [ ] GPU execution works if available\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Implement inference batching",
            "description": "Add batching for efficient model inference",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nBatching improves GPU utilization.\n\n## Requirements\n\n- Configurable batch size\n- Timeout for partial batches\n- Dynamic batching based on load\n\n## Acceptance Criteria\n\n- [ ] Batching improves throughput\n- [ ] Timeouts prevent stale data\n- [ ] Latency impact acceptable\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Implement model hot-swapping",
            "description": "Allow model replacement without pipeline restart",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nHot-swapping enables model updates without downtime.\n\n## Requirements\n\n- Load new model while old is serving\n- Atomic swap to new model\n- Graceful handling of in-flight requests\n\n## Acceptance Criteria\n\n- [ ] Model can be swapped at runtime\n- [ ] No requests lost during swap\n- [ ] Swap is atomic\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 14,
        "title": "Add Visualization Tools",
        "description": "Create tools for pipeline visualization, debugging, and interactive development.",
        "status": "pending",
        "priority": "low",
        "dependencies": [
          11
        ],
        "details": "## Context\n\nVisualization helps users understand and debug pipelines. Interactive tools improve developer experience.\n\n## Implementation Approach\n\n1. Generate pipeline DAG representation\n2. Create web-based visualization\n3. Add real-time data flow display\n4. Implement debug mode\n\n## Technical Considerations\n\n- Web UI technology choices\n- Real-time updates via WebSocket\n- Performance impact of visualization\n- Integration with IDE",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Pipeline DAG visualized correctly\n- [ ] Real-time data flow shown\n- [ ] Debug mode allows step-through\n- [ ] Performance profiling visualized\n- [ ] UI works in major browsers\n- [ ] Low overhead when not visualizing\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Generate pipeline DAG representation",
            "description": "Create data structure representing pipeline as DAG",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nDAG representation enables visualization.\n\n## Requirements\n\n- Capture all nodes and edges\n- Include node metadata\n- Export to common formats (DOT, JSON)\n\n## Acceptance Criteria\n\n- [ ] DAG captures pipeline structure\n- [ ] Metadata includes types, config\n- [ ] Export formats work\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Create web visualization UI",
            "description": "Build web-based pipeline visualization",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nWeb UI allows interactive exploration.\n\n## Requirements\n\n- Display pipeline DAG\n- Zoom and pan\n- Node details on click\n\n## Acceptance Criteria\n\n- [ ] Pipeline displayed clearly\n- [ ] Navigation is smooth\n- [ ] Node info accessible\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Add real-time data flow display",
            "description": "Show data flowing through pipeline in real-time",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nReal-time display helps understand pipeline behavior.\n\n## Requirements\n\n- Show throughput per node\n- Animate data flow\n- Highlight bottlenecks\n\n## Acceptance Criteria\n\n- [ ] Throughput displayed accurately\n- [ ] Animation is smooth\n- [ ] Bottlenecks visible\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Implement debug mode",
            "description": "Add step-through debugging for pipelines",
            "status": "pending",
            "dependencies": [
              2
            ],
            "details": "## Context\n\nDebug mode helps troubleshoot issues.\n\n## Requirements\n\n- Pause at breakpoints\n- Inspect data at each stage\n- Step forward/backward\n\n## Acceptance Criteria\n\n- [ ] Breakpoints work\n- [ ] Data inspection works\n- [ ] Stepping is reliable\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      },
      {
        "id": 15,
        "title": "Create Reusable Pipeline Components",
        "description": "Enable pipeline composition, templates, and reuse patterns for modular pipeline development.",
        "status": "pending",
        "priority": "low",
        "dependencies": [],
        "details": "## Context\n\nReusable components reduce duplication and enable sharing of common patterns. This is essential for large-scale pipeline development.\n\n## Implementation Approach\n\n1. Design component abstraction\n2. Implement pipeline templates\n3. Add serialization for sharing\n4. Create component registry\n\n## Technical Considerations\n\n- Type safety across components\n- Versioning and compatibility\n- Configuration injection\n- Testing reusable components",
        "testStrategy": "## Acceptance Criteria\n\n- [ ] Components can be composed into larger pipelines\n- [ ] Templates are parameterizable\n- [ ] Components can be serialized/deserialized\n- [ ] Component registry allows discovery\n- [ ] Type errors caught at composition time\n- [ ] Examples show composition patterns\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete",
        "subtasks": [
          {
            "id": 1,
            "title": "Design component abstraction",
            "description": "Create abstraction for reusable pipeline components",
            "status": "pending",
            "dependencies": [],
            "details": "## Context\n\nComponents are reusable pipeline fragments.\n\n## Requirements\n\n- Define component boundaries\n- Support typed inputs/outputs\n- Enable composition\n\n## Acceptance Criteria\n\n- [ ] Component trait defined\n- [ ] Composition is type-safe\n- [ ] API is ergonomic\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 2,
            "title": "Implement pipeline templates",
            "description": "Create parameterizable pipeline templates",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nTemplates allow customization of common patterns.\n\n## Requirements\n\n- Define template with parameters\n- Instantiate with specific values\n- Type-check parameters\n\n## Acceptance Criteria\n\n- [ ] Templates can be defined\n- [ ] Instantiation works correctly\n- [ ] Parameter errors caught early\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 3,
            "title": "Add component serialization",
            "description": "Enable serializing components for sharing",
            "status": "pending",
            "dependencies": [
              1
            ],
            "details": "## Context\n\nSerialization enables sharing and storage.\n\n## Requirements\n\n- Serialize to JSON/YAML\n- Deserialize and reconstruct\n- Version information\n\n## Acceptance Criteria\n\n- [ ] Round-trip serialization works\n- [ ] Versions handled correctly\n- [ ] Invalid configs rejected\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          },
          {
            "id": 4,
            "title": "Create component registry",
            "description": "Build registry for discovering components",
            "status": "pending",
            "dependencies": [
              3
            ],
            "details": "## Context\n\nRegistry enables component discovery.\n\n## Requirements\n\n- Register components by name\n- Search by type/tags\n- Version management\n\n## Acceptance Criteria\n\n- [ ] Components can be registered\n- [ ] Search works correctly\n- [ ] Versions resolved correctly\n- [ ] The command `bin/check && bin/lint && bin/build && bin/test` must successfully complete"
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-11-25T23:37:23.909Z",
      "updated": "2025-11-26T22:00:00.000Z",
      "description": "Tasks for master context"
    }
  }
}